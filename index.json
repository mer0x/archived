
[{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/categories/career/","section":"","summary":"","title":"Career","type":"categories"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/series/career/","section":"Series","summary":"","title":"Career","type":"series"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/tags/ccna/","section":"","summary":"","title":"CCNA","type":"tags"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/categories/certification/","section":"","summary":"","title":"Certification","type":"categories"},{"content":" My path from IT basics to becoming a certified networking professional through dedication, hands-on practice, and perseverance. From my first IT job to wanting to progress further didn\u0026rsquo;t take long. I knew I had relatively solid Linux fundamentals at that time for my first position, but I didn\u0026rsquo;t want to stay comfortable and let it become a boring routine. I started researching what knowledge I needed to become a complete sysadmin or even advance beyond that. That\u0026rsquo;s when I realized my networking foundation wasn\u0026rsquo;t quite zero, but weak enough that I didn\u0026rsquo;t even know what a VLAN was.\nTip: When planning your IT career path, don\u0026rsquo;t just focus on what you already know, identify your knowledge gaps and address them strategically. Finding the Right Learning Path # So I began planning my networking development path, looking for courses both online on platforms like Udemy and at vocational schools in my city that offered specialization courses up to CCNP level. If you\u0026rsquo;re from Timi»ôoara, I recommend checking out their website: https://savnet.ro/\nAfter completing an introductory networking course on Udemy, I called the team at Savnet, who nicely explained their curriculum, including physical labs - which convinced me immediately. The fact that I could work on industrial network equipment (routers, switches, firewalls) had me asking \u0026ldquo;Can we start today? :))\u0026rdquo;\nFirst day in front of the building where CCNA Module I began My CCNA Learning Evolution # Starting with Packet Tracer Module 1 From there to building my first homelab versions was just a small step. Initially, I installed Cisco Packet Tracer and started doing all the exercises I received in Savnet's lab plus homework assignments. I was so hyped that by the end of module 1, I was rushing to finish my exercises and personal troubleshooting sessions so I could help other students with their problems - probably one of the best ways to approach real IT troubleshooting scenarios. Advancing to GNS3 Module 2-3 During that period, my schedule was something like: 10 AM - 6 PM at my first job (which I wrote about here), and three times a week from 7 PM to 10 PM I attended CCNA labs. Even that wasn't enough for me - I had already developed a true passion. That's when I discovered GNS3, a complete emulator where I could use real router and switch images, as well as virtual machines and even Docker containers. That was the moment when my weekends started focusing largely on extensive practice in the virtual lab I created on my laptop. Complex Network Projects Module 4 Starting from module 3, we received much more complex projects that we had to complete at home and verify during labs with one of our instructors. CISCO equipment rack we worked on in the lab (AI Reworked) Random weekend day during the first CCNA modules Network Topologies \u0026amp; Lab Work # The exams at Savnet were challenging practical exercises, often more complex than the standard NetAcad curriculum to better prepare us. But my passion for networking extended beyond coursework - I spent countless hours building my own topologies and experimenting with different network configurations.\nCertification Day # After months of labs, theory, and various tests, the time came to schedule the actual certification exam. I\u0026rsquo;ve never been more nervous for any exam - I knew how important this certification had become for me. February 18, 2020, shortly before the COVID pandemic put the world on pause.\nCCNA exam 200-125 appointment I passed it on my first attempt! I had never felt such joy about an exam. The exam time was approximately 3 hours, containing both theoretical parts and labs with troubleshooting. I was so focused, engaged, and well-prepared that I remember submitting the final answers in about an hour and a half to learn my result. I could certainly have done better, but I never excel in theory as much as I do in practice. It was enough - it was the moment I\u0026rsquo;d been waiting for.\nFinal CCNA score (889/1000) The biggest bonus for my work during this period wasn\u0026rsquo;t just the certification, but the job offered to me shortly after in the Cybersecurity field - Blue team, through a recommendation from one of my former professors from CCNA module four. I\u0026rsquo;ll tell you about that job in a future blog post in this series.\nPersonal CCNA certificate Key Takeaways from My CCNA Journey # Start with fundamentals - Even if you think you know networking, solidify the basics Combine learning formats - Online courses, structured programs, and hands-on labs reinforce each other Build your own lab - Virtual environments like GNS3 or Packet Tracer accelerate learning Help others - Troubleshooting others\u0026rsquo; problems prepares you for real-world scenarios Certification pays off - The CCNA opened the door to my cybersecurity career Questions? # Are you considering pursuing your CCNA? Do you have questions about the certification process or how to prepare? Let me know in the comments!\n","date":"2 May 2025","externalUrl":null,"permalink":"/blog/ccna-career-boost/","section":"Blog","summary":"One year into my IT career, I decided to pursue the CCNA certification. Discover the reasons behind this choice and how it paved the way to my first role in cybersecurity.","title":"From Basic Skills to Networking Specialist: My CCNA Journey","type":"blog"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/tags/networking/","section":"","summary":"","title":"Networking","type":"tags"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/tags/professional-development/","section":"","summary":"","title":"Professional-Development","type":"tags"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/dual-boot/","section":"","summary":"","title":"Dual Boot","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/dual-boot-tutorial/","section":"","summary":"","title":"Dual Boot Tutorial","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/grub/","section":"","summary":"","title":"Grub","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/categories/linux/","section":"","summary":"","title":"Linux","type":"categories"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/linux/","section":"","summary":"","title":"Linux","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/categories/operating-systems/","section":"","summary":"","title":"Operating Systems","type":"categories"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/partitioning/","section":"","summary":"","title":"Partitioning","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/secure-boot/","section":"","summary":"","title":"Secure Boot","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/categories/tutorials/","section":"","summary":"","title":"Tutorials","type":"categories"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/ubuntu-25.04/","section":"","summary":"","title":"Ubuntu 25.04","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/ubuntu-installation/","section":"","summary":"","title":"Ubuntu Installation","type":"tags"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/uefi/","section":"","summary":"","title":"UEFI","type":"tags"},{"content":" Ultimate Dual Boot Guide: Windows11 and Ubuntu 25.04 # Ubuntu Plucky Puffin installer\nLast Updated: April 2025\nRunning both Windows 11 and Ubuntu 25.04 on the same computer gives you the best of both worlds. This step-by-step guide makes dual-booting simple, even for beginners!\nWhat Sets This Guide Apart # Unlike typical dual-boot tutorials, this guide offers:\nUEFI vs. Legacy BIOS instructions for modern computers Secure Boot compatibility solutions Performance optimization tips for both operating systems Advanced partition strategies for optimal system management Automated setup scripts to speed up post-installation configuration Virtualization options when dual-boot isn\u0026rsquo;t ideal Real-world use case scenarios to maximize your dual-boot experience What You\u0026rsquo;ll Need # A computer with Windows 11 already installed At least 30GB of free space (50GB+ recommended for comfortable usage) A USB drive (8GB or larger) About 30-45 minutes of your time Basic computer knowledge Part 1: Prepare Your Computer # 1. Back Up Your Data # Always back up important files before modifying your system.\nUse Windows built-in Backup and Restore Consider cloud backup solutions (OneDrive, Google Drive) For critical data, create an external drive backup 2. Check Your System Type: UEFI or Legacy BIOS # Press Win + R, type msinfo32, and press Enter Look for \u0026ldquo;BIOS Mode\u0026rdquo; under System Summary If it says \u0026ldquo;UEFI\u0026rdquo;, follow the UEFI instructions in this guide If it says \u0026ldquo;Legacy\u0026rdquo;, follow the Legacy instructions 3. Create Space for Ubuntu # Using Disk Management: # Press Win + X and select Disk Management Right-click on your largest partition (usually C:) and select Shrink Volume Enter the amount to shrink (minimum 30000 MB recommended for Ubuntu) For a more comfortable experience: 50000 MB (50GB) For developers/power users: 100000 MB (100GB) Click Shrink to create unallocated space Shrink your Windows partition to make room for Ubuntu\nAlternative: Using Disk Cleanup First (Recommended) # Before shrinking, free up space by:\nPress Win + R, type cleanmgr and press Enter Select your C: drive and click OK Click \u0026ldquo;Clean up system files\u0026rdquo; Select all items, especially \u0026ldquo;Windows Update Cleanup\u0026rdquo; and \u0026ldquo;Previous Windows installations\u0026rdquo; Click OK to reclaim gigabytes of space 4. Disable Fast Startup (Critical Step) # Go to Control Panel ‚Üí Power Options ‚Üí Choose what the power buttons do Click Change settings that are currently unavailable Uncheck Turn on fast startup Click Save changes Disabling Fast Startup prevents issues when dual-booting\n5. Disable BitLocker (If Enabled) # If you use BitLocker encryption:\nPress Win + X and select PowerShell (Admin) or Terminal (Admin) Type manage-bde -status to check BitLocker status If enabled, type manage-bde -off C: to decrypt your drive Wait for decryption to complete (may take hours) Note: You can re-enable BitLocker after Ubuntu installation, but it requires additional configuration.\n6. Create Recovery Drive (Recommended) # Search for \u0026ldquo;Create a recovery drive\u0026rdquo; in Windows Search Follow the wizard to create a Windows recovery USB Store it safely in case you need to restore Windows Part 2: Create Ubuntu Installation Media # 1. Download Ubuntu 25.04 # Download the Ubuntu 25.04 ISO file from ubuntu.com/download/desktop\nDownload the latest Ubuntu 25.04 ISO file\n2. Verify the ISO (For Extra Security) # Download the SHA256SUMS and SHA256SUMS.gpg files from the Ubuntu download page On Windows, open PowerShell and run: Get-FileHash -Algorithm SHA256 -Path path\\to\\ubuntu-25.04-desktop-amd64.iso Compare the output hash with the one in the SHA256SUMS file 3. Create Bootable USB # Using Rufus: # Download and install Rufus Insert your USB drive Open Rufus and select your USB drive Click SELECT and choose the Ubuntu ISO file For UEFI systems: Make sure \u0026ldquo;GPT\u0026rdquo; is selected in the partition scheme Click START and select Write in ISO Image mode Rufus will create a bootable Ubuntu USB drive\nAlternative: Using Ventoy (Multi-Boot Solution) # Download Ventoy Install it to your USB drive Copy the Ubuntu ISO directly to the USB You can add multiple ISOs to create a multi-boot USB Part 3: Install Ubuntu Alongside Windows # 1. Adjust UEFI/BIOS Settings # Restart your computer Enter BIOS/UEFI (usually by pressing F2, F12, Del, or Esc during startup) Make these critical changes: Disable \u0026ldquo;Secure Boot\u0026rdquo; (temporarily) Set \u0026ldquo;SATA Operation\u0026rdquo; to \u0026ldquo;AHCI\u0026rdquo; mode if using SSD Disable \u0026ldquo;Intel Rapid Storage Technology\u0026rdquo; if present Change boot order to prioritize USB Save and exit 2. Boot from USB # Restart your computer Press the boot menu key during startup (F12, F2, or Del - varies by computer) Select your USB drive from the boot menu On UEFI systems, select the \u0026ldquo;UEFI\u0026rdquo; entry for your USB 3. Start Ubuntu Installation # Select Try or Install Ubuntu Choose your language and click Install Ubuntu Select keyboard layout and click Continue For wireless connection, connect to your WiFi network 4. Choose Optimal Installation Type # For Beginners (Automatic Partitioning): # Select Install Ubuntu alongside Windows Boot Manager Click Install Now For Advanced Users (Manual Partitioning): # Select Something else for manual partitioning Create the following partitions in the unallocated space: EFI partition (if not already present): 512 MB, use as \u0026ldquo;EFI System Partition\u0026rdquo; Root partition (/): 20-30 GB, use as \u0026ldquo;Ext4\u0026rdquo;, mount point \u0026ldquo;/\u0026rdquo; Swap partition: Equal to your RAM (for hibernation support), use as \u0026ldquo;swap\u0026rdquo; Home partition (/home): Remaining space, use as \u0026ldquo;Ext4\u0026rdquo;, mount point \u0026ldquo;/home\u0026rdquo; 5. Confirm Partition Changes # Review the changes and click Continue\n6. Choose Your Location # Select your time zone on the map\n7. Create Your User Account # Enter your name, computer name, username, and password\nSecurity Tip: Use a different password than your Windows account\n8. Installation Process # Wait for the installation to complete (usually 10-15 minutes)\n9. Restart Your Computer # When prompted, remove the USB drive and click Restart Now\nPart 4: Post-Installation Configuration # 1. Re-enable Secure Boot (Optional) # If you want to use Secure Boot with Ubuntu:\nBoot into UEFI settings Find Secure Boot settings Enter \u0026ldquo;Setup Mode\u0026rdquo; if available Enable Secure Boot Save and exit Ubuntu 25.04 supports Secure Boot, but you may need to manage keys if you encounter boot issues.\n2. First Boot and Updates # At the GRUB menu, select Ubuntu Log in with your credentials Run system updates: sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y 3. Install Ubuntu Restricted Extras # For media codecs, fonts, and other proprietary software:\nsudo apt install ubuntu-restricted-extras 4. Install Hardware-Specific Drivers # For NVIDIA Graphics: # sudo ubuntu-drivers autoinstall For AMD Graphics: # The open-source drivers are usually included, but you can install the proprietary ones if needed:\nsudo add-apt-repository ppa:kisak/kisak-mesa sudo apt update \u0026amp;\u0026amp; sudo apt upgrade 5. Fix Time Synchronization Issues # To prevent time conflicts between Windows and Ubuntu:\nIn Ubuntu Terminal:\ntimedatectl set-local-rtc 1 --adjust-system-clock 6. Post-Installation Script (Exclusive to This Guide) # Save time with our automated setup script that configures:\nOptimal power settings Improved performance tweaks Common software installations Proper dual-boot time synchronization Create a file named dual-boot-setup.sh:\n#!/bin/bash # Update system sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y # Install essential software sudo apt install -y ubuntu-restricted-extras vlc gimp libreoffice timeshift gnome-tweaks # Fix time synchronization timedatectl set-local-rtc 1 --adjust-system-clock # Optimize SSD if present if [ -d \u0026#34;/sys/block/nvme0n1\u0026#34; ] || [ -d \u0026#34;/sys/block/sda\u0026#34; ]; then sudo apt install -y util-linux sudo systemctl enable fstrim.timer fi # Improve battery life sudo apt install -y tlp tlp-rdw sudo systemctl enable tlp # Set up auto-cleaning echo \u0026#39;APT::Periodic::Update-Package-Lists \u0026#34;1\u0026#34;; APT::Periodic::Download-Upgradeable-Packages \u0026#34;1\u0026#34;; APT::Periodic::AutocleanInterval \u0026#34;7\u0026#34;;\u0026#39; | sudo tee /etc/apt/apt.conf.d/20auto-upgrades # Performance improvements echo \u0026#39;vm.swappiness=10\u0026#39; | sudo tee -a /etc/sysctl.conf # Check and repair GRUB if needed sudo update-grub echo \u0026#34;Setup complete! Reboot for changes to take effect.\u0026#34; Make it executable and run:\nchmod +x dual-boot-setup.sh ./dual-boot-setup.sh Part 5: Using Your Dual-Boot System # 1. The GRUB Boot Menu # After restart, you\u0026rsquo;ll see the GRUB menu where you can select:\nUbuntu 25.04 Windows 11 The GRUB boot menu lets you choose which operating system to use\n2. Accessing Windows Files from Ubuntu # Ubuntu can read your Windows files:\nOpen Files in Ubuntu Look for your Windows drive in the sidebar Warning: Writing to NTFS partitions from Ubuntu may require additional setup:\nsudo apt install ntfs-3g 3. Accessing Ubuntu Files from Windows # To access Linux files from Windows 11:\nInstall WSL2 in Windows Install the WSL Ubuntu extension Use \\\\wsl$\\Ubuntu\\home\\yourusername in File Explorer Alternatively, install Paragon Linux File Systems for Windows\n4. Changing Default Operating System # To change which system boots by default:\nGraphical Method: # Install GRUB Customizer: sudo apt install grub-customizer Launch GRUB Customizer Go to \u0026ldquo;General Settings\u0026rdquo; tab Change \u0026ldquo;Default entry\u0026rdquo; to your preference Click Save Terminal Method: # In Ubuntu, open Terminal Type sudo nano /etc/default/grub Change GRUB_DEFAULT=0 to your preference (0 is usually Ubuntu) Set GRUB_TIMEOUT=10 for a longer selection time Press Ctrl+X, then Y to save Run sudo update-grub 5. Optimizing Performance in Dual-Boot Configuration # For Windows: # Disable indexing on drives shared with Linux Use Storage Sense to automatically free up space Disable unnecessary startup programs For Ubuntu: # Reduce swappiness for better performance: sudo echo \u0026#39;vm.swappiness=10\u0026#39; | sudo tee -a /etc/sysctl.conf Enable zRAM for better memory management: sudo apt install zram-config Part 6: Advanced Techniques and Alternatives # 1. Using Separate Hard Drives (Ideal Setup) # If your computer supports multiple drives:\nInstall Windows on first drive Install Ubuntu on second drive Configure BIOS/UEFI boot order or use boot menu to select OS Benefits:\nNo partition resizing needed Each OS gets a full drive Eliminates most dual-boot conflicts 2. Virtualization as Alternative # Windows as Host: # Enable virtualization in BIOS/UEFI Install WSL2 for Linux command-line: wsl --install Or install VirtualBox/VMware for full Ubuntu desktop Ubuntu as Host: # Install VirtualBox or GNOME Boxes: sudo apt install virtualbox Create Windows 11 VM (requires valid license) 3. Timeshift for System Backup # Create system snapshots before major changes:\nsudo apt install timeshift sudo timeshift --create --comments \u0026#34;Fresh Ubuntu installation\u0026#34; 4. Custom GRUB Theme (Make Your Dual-Boot Stylish) # Download a theme from GRUB Themes Extract the theme to /boot/grub/themes/ Edit GRUB configuration: sudo nano /etc/default/grub Add/modify: GRUB_THEME=\u0026quot;/boot/grub/themes/theme-name/theme.txt\u0026quot; Update GRUB: sudo update-grub Part 7: Real-World Use Cases # 1. Developer Workstation # Optimal configuration:\nWindows for Adobe Suite, Microsoft Office, and gaming Ubuntu for development (Docker, VS Code, programming languages) Shared data partition in exFAT format Git repositories on the Linux partition 2. Data Science Setup # Windows for Power BI and Excel analysis Ubuntu for Python, R, and machine learning frameworks Large data storage on separate drive accessible to both OSs Jupyter notebooks in shared folder 3. Gaming and Multimedia # Windows for AAA gaming titles Ubuntu for day-to-day browsing and work Steam installed on both systems with shared library folder Proton configured for Windows games on Linux Part 8: Troubleshooting Common Issues # Windows Not Showing in GRUB Menu # If Windows doesn\u0026rsquo;t appear in the boot menu:\nBoot into Ubuntu Open Terminal Type sudo os-prober Then sudo update-grub Ubuntu Won\u0026rsquo;t Boot After Windows Update # Windows updates may overwrite GRUB. To fix:\nBoot from Ubuntu USB in \u0026ldquo;Try Ubuntu\u0026rdquo; mode Open Terminal Run Boot Repair: sudo add-apt-repository ppa:yannubuntu/boot-repair sudo apt update sudo apt install boot-repair boot-repair Select \u0026ldquo;Recommended repair\u0026rdquo; Fixing Secure Boot Issues # If Ubuntu won\u0026rsquo;t boot with Secure Boot enabled:\nBoot into UEFI settings Disable Secure Boot temporarily Boot into Ubuntu Run: sudo apt install sbsigntool sudo update-secureboot-policy Recovering Windows Bootloader # If you need to restore Windows boot without Ubuntu:\nBoot from Windows installation media Select \u0026ldquo;Repair your computer\u0026rdquo; Go to Troubleshoot \u0026gt; Advanced Options \u0026gt; Command Prompt Run: bootrec /fixmbr bootrec /fixboot bootrec /rebuildbcd Part 9: Uninstalling Either OS (If Needed) # Removing Ubuntu while keeping Windows: # Boot into Windows Open Disk Management Delete the Ubuntu partitions Expand Windows partition Repair Windows boot using installation media Removing Windows while keeping Ubuntu: # Boot into Ubuntu Use GParted to delete Windows partitions: sudo apt install gparted sudo gparted Expand Ubuntu partitions as needed Update GRUB: sudo update-grub Conclusion # Congratulations! You now have a dual-boot system with Windows 11 and Ubuntu 25.04. Enjoy the flexibility of choosing between two powerful operating systems depending on your needs.\nRemember that Ubuntu offers amazing performance with lower system requirements than Windows, making it perfect for:\nProgramming Web development Office work Multimedia Gaming (with Steam\u0026rsquo;s Proton compatibility layer) This dual-boot setup gives you the freedom to use the best tool for each job, while our performance optimizations ensure both systems run at their best.\nRecommended Next Steps # Set up cloud synchronization across both OSs (Dropbox, OneDrive, etc.) Configure SSH keys and development environments in Ubuntu Create your ideal productivity workflow between the two systems Explore Linux gaming with Proton and Steam If you have any questions, need help with your specific hardware, or want to share your dual-boot experience, leave a comment below!\n","date":"14 April 2025","externalUrl":null,"permalink":"/blog/windows-11-ubuntu-25-04-dual-boot-guide/","section":"Blog","summary":"Complete step-by-step guide for installing and configuring a dual boot system with Windows 11 and Ubuntu 25.04, perfect for beginners and advanced users alike.","title":"Ultimate Dual Boot Guide: Windows 11 and Ubuntu 25.04","type":"blog"},{"content":"","date":"14 April 2025","externalUrl":null,"permalink":"/tags/windows-11/","section":"","summary":"","title":"Windows 11","type":"tags"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/tags/cloudflare/","section":"","summary":"","title":"Cloudflare","type":"tags"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/categories/devops/","section":"","summary":"","title":"DevOps","type":"categories"},{"content":"Ten years ago, I started my first website projects‚Äîsome for myself, others for friends or clients. I spent a lot of time experimenting with themes, plugins, and WordPress setups. Some of the projects I dedicated the most effort to are listed here: https://merox.dev/websites/\nüß± My Experience with WordPress # WordPress is a solid way to get started‚Äîespecially if you‚Äôre messing around with your first LAMP server. But long term? It can be a pain. You‚Äôll find yourself tweaking PHP configs, switching to LiteSpeed, juggling plugin updates, and basically fine-tuning everything so it doesn‚Äôt fall apart.\n‚úÖ What I Liked # Database-based structure Huge plugin ecosystem Easy integrations with third-party tools Pretty smooth for non-devs thanks to the admin dashboard ‚ùå But Also\u0026hellip; # Every plugin is another potential vulnerability Too many resources for something like a static website Gets heavy fast (plugins + DB = bottleneck) Needs constant updates and babysitting From my perspective, if you just want a simple presentation website, WordPress is overkill. You\u0026rsquo;re burning CPU cycles and adding surface area for no real reason.\nE-commerce? Maybe. WooCommerce is fine‚Äîuntil it isn‚Äôt. Do your research. Shopify is probably the better call long-term.\nHere‚Äôs a visual from wpscan.com showing the current number of known vulnerabilities: üåÄ Moving to Hugo # For over 6 months now, I‚Äôve been all-in on Hugo‚Äîmy current site merox.dev runs entirely on it. I had totally underestimated the power of a static site. Hugo‚Äôs written in Go, builds super fast, and is already SEO-optimized out of the box.\nI chose Blowfish as the theme and tweaked it over time to fit my needs.\n‚úÖ Hugo Pros # Static = zero backend vuln headaches Hosting? Free, thanks to GitHub Pages Markdown = clean, portable, fast to edit Insanely fast loading Version control with Git, so I always know what‚Äôs changing ‚ùå Hugo Cons # You need to be a bit technical No dashboard‚Äîeverything‚Äôs in Markdown Some DevOps skills help if you want a clean, automated setup üß© Setting Up Hugo + Blowfish # Install Hugo: https://gohugo.io/installation/ Add Blowfish theme: https://blowfish.page/docs/installation/ Use blowfish-tools for live previews‚Äîit‚Äôs a nice touch To keep the theme up to date without breaking my customizations, I forked Blowfish and added it as a Git submodule like this:\ngit submodule add https://github.com/YOURUSERNAME/blowfish themes/blowfish This way, I can track upstream updates while maintaining my own style.\nüåê Connecting Domain via GitHub Pages + Cloudflare # After installing Hugo + Blowfish, I decided to host my site using GitHub Pages and manage DNS via Cloudflare.\nHere‚Äôs a quick guide:\nPush Hugo project to a private repo Enable GitHub Pages on a public repo (choose branch: gh-pages, folder: /) In Cloudflare: Add a CNAME for www.yourdomain.com ‚Üí yourusername.github.io Add A records pointing to GitHub Pages IPs if needed Add a CNAME file in your repo containing: merox.dev Wait for DNS to propagate and you‚Äôre done.\n‚öôÔ∏è Automating Deploys with GitHub Actions # I‚Äôm not about that manual deploy life. So I set up GitHub Actions to take care of builds and deploys whenever I push to master.\nMy workflow? Make changes in VS Code ‚Üí push to private repo ‚Üí GitHub Actions builds ‚Üí deploys to public repo.\nüîê Connecting Private + Public Repo Using RSA # I didn‚Äôt want my source repo to be public. So here‚Äôs how I did the private ‚Üí public deploy using RSA keys:\nSSH Deploy Setup # Generate an RSA key pair: ssh-keygen -t rsa -b 4096 -C \u0026#34;github-deploy\u0026#34; -f deploy_key -N \u0026#34;\u0026#34; Public repo (destination):\nGo to Settings ‚Üí Deploy Keys ‚Üí Add deploy_key.pub with write access Private repo (source):\nSettings ‚Üí Secrets ‚Üí Add PRIVATE_KEY ‚Üí paste the private key contents My .github/workflows/deploy.yml:\n1name: github pages 2 3on: 4 push: 5 branches: 6 - master 7 8permissions: 9 contents: write 10 id-token: write 11 12jobs: 13 deploy: 14 runs-on: ubuntu-22.04 15 steps: 16 - uses: actions/checkout@v4 17 with: 18 submodules: true 19 fetch-depth: 0 20 21 - name: Setup Hugo 22 uses: peaceiris/actions-hugo@v3 23 with: 24 hugo-version: \u0026#39;latest\u0026#39; 25 extended: true 26 27 - name: Build 28 run: hugo --minify 29 30 - name: Deploy 31 uses: peaceiris/actions-gh-pages@v3 32 with: 33 deploy_key: ${{ secrets.PRIVATE_KEY }} 34 external_repository: mer0x/merox.dev 35 publish_branch: gh-pages 36 publish_dir: ./public üìà SEO Results After the Switch # I didn‚Äôt even have to do much SEO magic ‚Äî Hugo just works. With clean structure and no bloat, I got over 80% SEO scores right away. That‚Äôs the beauty of static sites done right.\nFrom https://freetools.seobility.net/\nFrom https://seositecheckup.com/\nüí¨ Final Thoughts # No more updates every week. No plugin bugs. No servers to worry about. And I finally get a blazing-fast site, version-controlled, with everything set up exactly how I want.\nIf you‚Äôre into the idea of a static site and want help setting one up‚Äîor just have questions‚Äîdrop me an email or comment. Happy to help.\n","date":"7 April 2025","externalUrl":null,"permalink":"/blog/wordpress-to-hugo-setup/","section":"Blog","summary":"Why I migrated from WordPress to Hugo, pros and cons of both platforms, and a detailed look into how my website runs today using GitHub Actions, GitHub Pages, and Cloudflare.","title":"From WordPress to Hugo: My Setup Explained","type":"blog"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/tags/github-actions/","section":"","summary":"","title":"Github Actions","type":"tags"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/tags/github-pages/","section":"","summary":"","title":"Github Pages","type":"tags"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/tags/hugo/","section":"","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/categories/infrastructure/","section":"","summary":"","title":"Infrastructure","type":"categories"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/tags/static-site/","section":"","summary":"","title":"Static Site","type":"tags"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/categories/web-development/","section":"","summary":"","title":"Web Development","type":"categories"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/tags/website-setup/","section":"","summary":"","title":"Website Setup","type":"tags"},{"content":"","date":"7 April 2025","externalUrl":null,"permalink":"/tags/wordpress/","section":"","summary":"","title":"Wordpress","type":"tags"},{"content":"","date":"17 March 2025","externalUrl":null,"permalink":"/tags/google/","section":"","summary":"","title":"Google","type":"tags"},{"content":"","date":"17 March 2025","externalUrl":null,"permalink":"/tags/innovation/","section":"","summary":"","title":"Innovation","type":"tags"},{"content":" The reception area of Google\u0026rsquo;s Warsaw HUB Source: Google Warsaw HUB Reception Image. The image is external, as I forgot to take a photo of the reception myself.\nA Business Trip Turned Into a Tech Adventure # My trip to Warsaw originally revolved around discussions on the design of HPC (High-Performance Computing) infrastructure at my company. However, it also turned into an exciting experience when a friend working at Google invited me to visit their Warsaw HUB offices. This was an opportunity I couldn‚Äôt pass up!\nGoogle‚Äôs Expansion in Warsaw # Google has heavily invested in Poland, establishing Google Cloud\u0026rsquo;s first Central and Eastern European region in Warsaw. The Google Warsaw HUB spans 14 floors of the Warsaw HUB skyscraper complex, reinforcing the city\u0026rsquo;s status as a key tech hub in Europe. (source)\nStepping Into Google‚Äôs Office # Upon arrival, I received a guest badge, which allowed me to move freely within designated areas.\nMy guest badge at Google Warsaw HUB. The Elevators: More Than Just Transportation # The elevator hall was eye-catching, illuminated in Google‚Äôs signature colors. Inside the elevator, a screen displayed the building‚Äôs floor plan, showing which Google departments were located at each level.\nThe elevator hall, decorated in Google\u0026rsquo;s signature colors. Inside the elevator, showing department locations on each floor. The Coffee Corner: A Relaxing Break Area # One of the standout spaces was the coffee corner, a relaxed environment where employees could grab free snacks and drinks. The space even had mixing stations with DJ equipment!\nA cozy coffee corner for relaxation and casual work discussions. The Restaurant: A Missed Opportunity # Google provides free meals for employees and, on some occasions, even for visitors. Unfortunately, by the time I got there, the restaurant had closed for the day.\nOne of Google\u0026rsquo;s internal restaurants at Warsaw HUB. The View from the 30th Floor # Reaching the 30th floor, I was greeted by an incredible panoramic view of Warsaw‚Äôs skyline, highlighting the city\u0026rsquo;s technological district.\nA stunning view of Warsaw\u0026rsquo;s tech district from Google‚Äôs Warsaw HUB. Work and Play: Unique Office Amenities # Google‚Äôs focus on employee well-being was evident throughout the office. Among the highlights:\nPower nap rooms for quick relaxation. Gaming rooms featuring both modern and retro games. A fully-equipped gym on the top floor, available to all employees. Conclusion: A Workplace Designed for Productivity and Comfort # Visiting Google‚Äôs Warsaw HUB was an eye-opening experience. The combination of cutting-edge technology, modern workspace design, and thoughtful employee perks made it clear why Google is considered one of the best places to work.\nSpecial Thanks # I\u0026rsquo;d like to express my gratitude to Dorian Verna for making this visit possible and showing me around Google\u0026rsquo;s impressive workspace.\n","date":"17 March 2025","externalUrl":null,"permalink":"/blog/google-hub-visit/","section":"Blog","summary":"During my business trip to Warsaw, I had the unique opportunity to visit Google\u0026rsquo;s impressive offices. Here\u0026rsquo;s an inside look at their workspace, from panoramic views to relaxation zones.","title":"Inside Google's Offices: A Tour of Innovation and Comfort","type":"blog"},{"content":"","date":"17 March 2025","externalUrl":null,"permalink":"/categories/tech-culture/","section":"","summary":"","title":"Tech Culture","type":"categories"},{"content":"","date":"17 March 2025","externalUrl":null,"permalink":"/tags/work-environment/","section":"","summary":"","title":"Work Environment","type":"tags"},{"content":"","date":"1 March 2025","externalUrl":null,"permalink":"/tags/backup/","section":"","summary":"","title":"Backup","type":"tags"},{"content":"","date":"1 March 2025","externalUrl":null,"permalink":"/categories/backup--recovery/","section":"","summary":"","title":"Backup \u0026 Recovery","type":"categories"},{"content":"","date":"1 March 2025","externalUrl":null,"permalink":"/tags/disaster-recovery/","section":"","summary":"","title":"Disaster Recovery","type":"tags"},{"content":"In today‚Äôs digital landscape, data loss can be catastrophic whether you‚Äôre running a sophisticated homelab or managing IT for an organization of any size. This guide shares my personal disaster recovery strategy, incorporating industry best practices and security considerations to help you build resilience against potential failures.\nCommon Backup Mistakes and Ransomware Risks # Even with a structured backup plan, common mistakes can make backups ineffective when disaster strikes. ‚ùå Frequent Backup Mistakes # Lack of Backup Testing ‚Äì A backup is useless if you‚Äôve never tested restoring from it. Storing Backups on the Same System ‚Äì Backups stored on the same machine or network are vulnerable to failures, ransomware, and accidental deletion. Unencrypted Backups ‚Äì Without encryption, your backups can be easily compromised. Overwriting Previous Backups ‚Äì Without versioning, ransomware or file corruption can render all backup copies useless. No Offsite Backup ‚Äì Only keeping local copies increases risk in case of fire, theft, or natural disasters. üî• Ransomware \u0026amp; Backup Protection # Modern ransomware attacks actively seek and encrypt backup files, making recovery impossible unless preventive measures are in place. To mitigate these threats:\nüõ° Implement Backup Protections PBS allows marking snapshots as \u0026lsquo;protected\u0026rsquo;, preventing accidental deletion. Note: Users with sufficient permissions can still remove this protection. More details: Proxmox Forum Discussion üîå Air-Gapped Backups ‚Äì Maintain at least one backup that is offline or isolated from the network. üîë Enable Multi-Factor Authentication (MFA) ‚Äì Restrict access to backup systems to prevent unauthorized tampering. üìä Set Up Alerts for Backup Failures ‚Äì Ensure you\u0026rsquo;re notified immediately when a backup job fails, so you can take action. By addressing these risks, you can ensure your backups remain resilient against both accidental failures and cyber threats. My Infrastructure Stack # Core Components # Primary Storage: Synology DS223 with 2x2TB drives in RAID1 configuration Backup Server: Proxmox Backup Server (PBS) running with 4x Intel D3-S4510 SSDs in RAIDz2 Offsite Storage: Hetzner VPS with attached StorageBox for geographical redundancy Secure Key Management: Local KeyPass on iOS/MacOS containing encryption keys The 3-2-1 Backup Strategy in Action # I\u0026rsquo;ve implemented the widely-recommended 3-2-1 backup approach:\n3 copies of data (original + 2 backups) 2 different storage types (local NAS and cloud storage) 1 offsite copy (Hetzner StorageBox) Backup Flow \u0026amp; Schedule # My automated backup chain ensures data flows through the system with minimal intervention:\nVM/LXC ‚Üí PBS: Every Saturday at 02:00\nPrimary backup of all virtual machines and containers Encrypted at rest for security PBS ‚Üí Synology: Every Saturday at 06:00\nSecondary local copy using rsync and crontab RAID1 protection against single drive failure Synology ‚Üí Hetzner: Every Saturday at 08:00\nOffsite copy for geographic redundancy Protection against local disasters (fire, theft, etc.) Implementation Details # Critical Scripts for Backup Automation # PBS to Synology Rsync (Running on PBS Server) # 0 6 * * 6 rsync -av --delete --progress /mnt/datastore/ /mnt/hyperbackup/ \u0026gt;\u0026gt; /var/log/rsync_backup.log 2\u0026gt;\u0026amp;1 More info about these scripts can be found here Proxmox Backup Client Script (backup-pbs.sh) # #!/bin/bash # 1) Export token secret as \u0026#34;PBS_PASSWORD\u0026#34; export PBS_PASSWORD=\u0026#39;token-secret-from-PBS\u0026#39; # 2) Define user@pbs + token export PBS_USER_STRING=\u0026#39;token-id-from-PBS\u0026#39; # 3) PBS IP/hostname export PBS_SERVER=\u0026#39;PBS-IP\u0026#39; # 4) Datastore name export PBS_DATASTORE=\u0026#39;DATASTORE_PBS\u0026#39; # 5) Build complete repository export PBS_REPOSITORY=\u0026#34;${PBS_USER_STRING}@${PBS_SERVER}:${PBS_DATASTORE}\u0026#34; # 6) Get local server shortname export PBS_HOSTNAME=\u0026#34;$(hostname -s)\u0026#34; # 7) ENCRYPTION KEY export PBS_KEYFILE=\u0026#39;/root/pbscloud_key.json\u0026#39; echo \u0026#34;Run pbs backup for $PBS_HOSTNAME ...\u0026#34; proxmox-backup-client backup \\ srv.pxar:/srv \\ volumes.pxar:/var/lib/docker/volumes \\ netw.pxar:/var/lib/docker/network \\ etc.pxar:/etc \\ scripts.pxar:/usr/local/bin \\ --keyfile /root/pbscloud_key.json \\ --skip-lost-and-found \\ --repository \u0026#34;$PBS_REPOSITORY\u0026#34; # List existing backups proxmox-backup-client list --repository \u0026#34;${PBS_REPOSITORY}\u0026#34; echo \u0026#34;Done.\u0026#34; Proxmox Backup Client Restore Script (backup-pbs-restore.sh) # #!/bin/bash # Global configs export PBS_PASSWORD=\u0026#39;token-secret-from-PBS\u0026#39; export PBS_USER_STRING=\u0026#39;token-id-from-PBS\u0026#39; export PBS_SERVER=\u0026#39;PBS_IP\u0026#39; export PBS_DATASTORE=\u0026#39;DATASTORE_FROM_PBS\u0026#39; export PBS_KEYFILE=\u0026#39;/root/pbscloud_key.json\u0026#39; export PBS_REPOSITORY=\u0026#34;${PBS_USER_STRING}@${PBS_SERVER}:${PBS_DATASTORE}\u0026#34; # Input parameters SNAPSHOT_PATH=\u0026#34;$1\u0026#34; ARCHIVE_NAME=\u0026#34;$2\u0026#34; RESTORE_DEST=\u0026#34;$3\u0026#34; # Parameter validation if [[ -z \u0026#34;$SNAPSHOT_PATH\u0026#34; || -z \u0026#34;$ARCHIVE_NAME\u0026#34; || -z \u0026#34;$RESTORE_DEST\u0026#34; ]]; then echo \u0026#34;Usage: $0 \u0026lt;snapshot_path\u0026gt; \u0026lt;archive_name\u0026gt; \u0026lt;destination\u0026gt;\u0026#34; echo \u0026#34;Example: $0 \\\u0026#34;host/cloud/2025-01-22T15:19:17Z\\\u0026#34; srv.pxar /root/restore-srv\u0026#34; exit 1 fi # Create destination if needed mkdir -p \u0026#34;$RESTORE_DEST\u0026#34; # Summary display echo \u0026#34;=== PBS Restore ===\u0026#34; echo \u0026#34;Snapshot: $SNAPSHOT_PATH\u0026#34; echo \u0026#34;Archive: $ARCHIVE_NAME\u0026#34; echo \u0026#34;Destination: $RESTORE_DEST\u0026#34; echo \u0026#34;Repository: $PBS_REPOSITORY\u0026#34; echo \u0026#34;Encryption key $PBS_KEYFILE\u0026#34; echo \u0026#34;=====================\u0026#34; # Run restore proxmox-backup-client restore \\ \u0026#34;$SNAPSHOT_PATH\u0026#34; \\ \u0026#34;$ARCHIVE_NAME\u0026#34; \\ \u0026#34;$RESTORE_DEST\u0026#34; \\ --repository \u0026#34;$PBS_REPOSITORY\u0026#34; \\ --keyfile \u0026#34;$PBS_KEYFILE\u0026#34; EXIT_CODE=$? if [[ $EXIT_CODE -eq 0 ]]; then echo \u0026#34;=== Restore completed successfully! ===\u0026#34; else echo \u0026#34;Restore error (code $EXIT_CODE).\u0026#34; fi exit $EXIT_CODE Disaster Recovery Scenarios # Having a backup is only half the solution‚Äîknowing how to restore is equally critical. Here are my documented procedures for various failure scenarios:\nScenario 1: Synology NAS Failure # Even if my primary NAS fails, data remains safe in two locations:\nProxmox Backup Server (4x Intel SSDs in RAIDz2) Hetzner StorageBox (offsite) Recovery Steps:\nReplace the failed hardware components Reconfigure RAID1 on the new or repaired NAS Restore HyperBackup schedule (targeting Saturday 08:00) Verify successful completion of first backup cycle Scenario 2: Hetzner VPS/StorageBox Failure # If my cloud provider experiences issues:\nProvision a new VPS with appropriate specifications Install proxmox-backup-client: For Ubuntu: Follow the community guide For Debian: Use standard package installation methods Create the encryption key file at /root/pbscloud_key.json: Retrieve the key from KeyPass (stored on iOS/MacOS) Deploy backup automation scripts: backup-pbs.sh for regular backups backup-pbs-restore.sh for potential recoveries Test both backup and restore functionality to verify operations Restore crontab for automatically backup: 0 2 * * * /usr/local/bin/backup-pbs.sh \u0026gt;\u0026gt; /var/log/backup-cloud.log 2\u0026gt;\u0026amp;1 Scenario 3: PBS Server Failure # In case my primary backup server fails:\nDownload and install the latest PBS ISO Configure storage properly: # /etc/proxmox-backup/datastore.cfg datastore: raidz2 comment gc-schedule sat 03:30 notification-mode notification-system path /mnt/datastore Verify /etc/fstab contains correct mount point: #raidz2 /dev/sdb /mnt/datastore ext4 defaults 0 2 Note: /dev/sdb represents the RAIDz2 array ( because in this scenarion the PBS it\u0026rsquo;s a VM and /dev/sdb it\u0026rsquo;s a second disk attached from RAIDz2 pool )\nEnsure the datastore has the required structure:\n.chunks vm .gc-status ct host Data can be restored from multiple sources:\nOriginal RAIDz2 array (if drives survived) Hetzner StorageBox (/mnt/storagebox/Storage_1) Synology NAS (/volume1/Backup/Proxmox/hyperbackup) Import VM/LXC encryption key from KeyPass into the new PVE environment.\nSecurity Best Practices # Based on my experience, here are critical security measures for robust disaster recovery:\nEncryption Throughout the Chain # Data-at-Rest Encryption: All my backups are encrypted using strong keys Transport Encryption: Using secure SSH tunnels for data transfer Key Management: Isolated storage of encryption keys in KeyPass Regular Key Rotation: Changing encryption keys periodically Access Control # Principle of Least Privilege: Backup systems have minimal permissions Token-Based Authentication: Using secure tokens rather than passwords Network Segmentation: Backup systems on separate network segments Firewall Rules: Strict ingress/egress rules for backup traffic Critical Files and Keys # Always securely store:\nEncryption keys in KeyPass (iOS/MacOS): /root/pbscloud_key.json PBS VM/LXC encryption key PBS Configuration: /etc/proxmox-backup/datastore.cfg Backup location references: PBS: /mnt/datastore Synology: /volume1/Backup/Proxmox/hyperbackup Hetzner: /mnt/storagebox/Storage_1 Continuous Improvement Recommendations # No backup system is perfect without ongoing validation and improvement. Here are practices I\u0026rsquo;m implementing or planning to adopt: ‚úÖ Regular Backup Verification # üîÑ Monthly integrity checks on random files üîç Checksum validation to detect bit rot üìä Log analysis for backup completion and failures üõ† Automated Recovery Testing # üîÑ Quarterly test restores to verify recoverability üìú Documented results with timing measurements üéØ Improvement targets based on test results üîî Monitoring and Alerting # üì° Real-time monitoring of backup processes ‚ö† Alert systems for backup failures or delays üìâ Storage capacity trend analysis to prevent space issues üìñ Documentation and Training # üìë Keeping recovery documentation updated üîÑ Regular practice of recovery procedures üë• Cross-training to ensure multiple people can perform recovery üîê Security Updates # üîÑ Regular patching of backup systems üõ° Vulnerability scanning of the backup infrastructure üîë Updating encryption standards as needed Conclusion # Disaster recovery isn\u0026rsquo;t just about having backups‚Äîit\u0026rsquo;s about having a proven, tested strategy that can be executed confidently when needed. For homelabbers and businesses alike, the approach outlined here provides a solid foundation for data protection without enterprise-level budgets.\nBy implementing proper backup chains, documenting recovery procedures, and regularly testing your systems, you can achieve peace of mind knowing your critical data can survive:\n‚úî Hardware failures\n‚úî Human errors\n‚úî Malicious attacks\nüí¨ What disaster recovery strategies do you use in your environment?\nI\u0026rsquo;d love to hear your thoughts and experiences in the comments below! üöÄ # Disclaimer: This approach works for my specific needs but should be adapted to your unique requirements. Always test your recovery procedures thoroughly before relying on them in an actual disaster scenario.\n","date":"1 March 2025","externalUrl":null,"permalink":"/blog/3-2-1-backup-strategy/","section":"Blog","summary":"In today‚Äôs digital landscape, data loss can be catastrophic whether you‚Äôre running a sophisticated homelab or managing IT for an organization of any size.","title":"Disaster Recovery Guide: My Approach to Safeguarding Critical Data","type":"blog"},{"content":"","date":"1 March 2025","externalUrl":null,"permalink":"/tags/hetzner/","section":"","summary":"","title":"Hetzner","type":"tags"},{"content":"","date":"1 March 2025","externalUrl":null,"permalink":"/tags/homelab/","section":"","summary":"","title":"Homelab","type":"tags"},{"content":"","date":"1 March 2025","externalUrl":null,"permalink":"/tags/proxmox/","section":"","summary":"","title":"Proxmox","type":"tags"},{"content":"","date":"1 March 2025","externalUrl":null,"permalink":"/tags/synology/","section":"","summary":"","title":"Synology","type":"tags"},{"content":"","date":"11 February 2025","externalUrl":null,"permalink":"/tags/automation/","section":"","summary":"","title":"Automation","type":"tags"},{"content":"","date":"11 February 2025","externalUrl":null,"permalink":"/categories/cloud--self-hosting/","section":"","summary":"","title":"Cloud \u0026 Self-Hosting","type":"categories"},{"content":"It‚Äôs already been a year since my first Kubernetes journey. My initial clusters‚Äîwhere I started learning and understanding more about Kubernetes‚Äîare now all taken down. This time, I want to build a fully functional, highly available (HA) cluster.\nOver the past weeks, I‚Äôve done more research in Kubernetes communities, as well as on subreddits like [kubernetes], [homelab], and [selfhosted]. I discovered that one of the best ways to deploy a cluster these days is by following guides and content from Techno Tim, so I decided to write this blog and share my own approach.\nTip: If you‚Äôre new to K3s, subreddits like r/kubernetes and r/homelab can be great resources to learn from fellow enthusiasts. What I Want to Achieve # A fully organized HA cluster on my hardware, so if any of my machines go down, the cluster remains functional. Specifically: 1 x DELL R720 ‚Üí k3s-master-1 and k3s-worker-1 1 x DELL Optiplex Micro 3050 ‚Üí k3s-master-2 and k3s-worker-2 1 x DELL Optiplex Micro 3050 ‚Üí k3s-master-3 and k3s-worker-3 How I Will Deploy # I will create six virtual machines (VMs) on a Proxmox cluster:\n3 x Ubuntu 22.04 Master Nodes 3 x Ubuntu 22.04 Worker Nodes The goal is to run K3s on these VMs to set up a solid Kubernetes environment with redundancy.\nLet‚Äôs Begin! # In the upcoming sections, I‚Äôll detail each step, from setting up Proxmox VMs to installing and configuring K3s, managing networking, storage, and beyond.\nChapter 1: Preparing DNS and IP Addresses # When setting up a Kubernetes cluster, DNS and IP management are crucial. Below is how I handle DHCP, static IP assignments, and DNS entries in my homelab environment.\nDHCP Configuration # There are two possible scenarios for assigning IP addresses to your VMs:\nUse IP addresses outside of your DHCP range\nThis method is often preferred, as your machines will keep their manually configured network settings even if your DHCP server goes down.\nDHCP Static Mappings\nYou can map MAC -\u0026gt; IP in your network services to allocate IP addresses to VMs based on their MAC addresses.\nTip: If you choose the second scenario, make sure you document your static leases carefully. Proper documentation avoids conflicts and confusion later.\nMy Approach # I chose the first scenario, where I use IPs outside the DHCP range. This ensures my network remains stable if the DHCP service is unavailable.\nIP Range: 10.57.57.30/24 ‚Üí 10.57.57.35/24 for my VMs DNS Setup # I also set up a DNS entry in my Unbound service on pfSense to easily manage and access my machines. For instance, you can create an A record or similar DNS record type pointing to your VM‚Äôs IP address. Below is a simple example:\nChapter 2: Automated VM Deployment on Proxmox with Cloud-Init # To streamline the next steps, I‚Äôve created a bash script that automates crucial parts of the process, including:\nCreating a Cloud-Init template Deploying multiple VMs with static IP addresses Destroying the VMs if needed If you prefer an even more automated approach using tools like Packer or Terraform, I suggest checking out this related post: Homelab as Code and adapting it to your specific scenario. However, for this blog, I‚Äôll demonstrate a simpler, more direct approach using the script below.\nWarning\nThis script can create or destroy VMs. Use it carefully and always keep backups of critical data. Prerequisites # Make sure you have Proxmox up and running. You‚Äôll need to place your SSH public key (e.g., /root/.ssh/id_rsa.pub) on the Proxmox server before running the script. Script Overview # Option 1: Create Cloud-Init Template\nDownloads the Ubuntu Cloud image (currently Ubuntu 24.04, code-named ‚Äúnoble‚Äù) Creates a VM based on the Cloud-Init image Converts it into a template Option 2: Deploy VMs\nClones the Cloud-Init template to create the desired number of VMs Configures IP addressing, gateway, DNS, search domain, SSH key, etc. Adjusts CPU, RAM, and disk size to fit your needs Option 3: Destroy VMs\nStops and removes VMs created by this script During the VM creation process, you‚Äôll be prompted to enter the VM name for each instance (e.g., k3s-master-1, k3s-master-2, etc.).\nTip\nTo fully automate naming, you could edit the script to increment VM names automatically. However, prompting ensures you can organize VMs with custom naming. The Bash Script # Below is the full script. Feel free to customize it based on your storage, networking, and naming preferences.\n1#!/bin/bash 2 3# Function to get user input with a default value 4get_input() { 5 local prompt=$1 6 local default=$2 7 local input 8 read -p \u0026#34;$prompt [$default]: \u0026#34; input 9 echo \u0026#34;${input:-$default}\u0026#34; 10} 11 12# Ask the user whether they want to create a template, deploy or destroy VMs 13echo \u0026#34;Select an option:\u0026#34; 14echo \u0026#34;1) Create Cloud-Init Template\u0026#34; 15echo \u0026#34;2) Deploy VMs\u0026#34; 16echo \u0026#34;3) Destroy VMs\u0026#34; 17read -p \u0026#34;Enter your choice (1, 2, or 3): \u0026#34; ACTION 18 19if [[ \u0026#34;$ACTION\u0026#34; != \u0026#34;1\u0026#34; \u0026amp;\u0026amp; \u0026#34;$ACTION\u0026#34; != \u0026#34;2\u0026#34; \u0026amp;\u0026amp; \u0026#34;$ACTION\u0026#34; != \u0026#34;3\u0026#34; ]]; then 20 echo \u0026#34;‚ùå Invalid choice. Please run the script again and select 1, 2, or 3.\u0026#34; 21 exit 1 22fi 23 24# === OPTION 1: CREATE CLOUD-INIT TEMPLATE === 25if [[ \u0026#34;$ACTION\u0026#34; == \u0026#34;1\u0026#34; ]]; then 26 TEMPLATE_ID=$(get_input \u0026#34;Enter the template VM ID\u0026#34; \u0026#34;300\u0026#34;) 27 STORAGE=$(get_input \u0026#34;Enter the storage name\u0026#34; \u0026#34;local\u0026#34;) 28 TEMPLATE_NAME=$(get_input \u0026#34;Enter the template name\u0026#34; \u0026#34;ubuntu-cloud\u0026#34;) 29 IMG_URL=\u0026#34;https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img\u0026#34; 30 IMG_FILE=\u0026#34;/root/noble-server-cloudimg-amd64.img\u0026#34; 31 32 echo \u0026#34;üì• Downloading Ubuntu Cloud image...\u0026#34; 33 cd /root 34 wget -O $IMG_FILE $IMG_URL || { echo \u0026#34;‚ùå Failed to download the image\u0026#34;; exit 1; } 35 36 echo \u0026#34;üñ•Ô∏è Creating VM $TEMPLATE_ID...\u0026#34; 37 qm create $TEMPLATE_ID --memory 2048 --cores 2 --name $TEMPLATE_NAME --net0 virtio,bridge=vmbr0 38 39 echo \u0026#34;üíæ Importing disk to storage ($STORAGE)...\u0026#34; 40 qm disk import $TEMPLATE_ID $IMG_FILE $STORAGE || { echo \u0026#34;‚ùå Failed to import disk\u0026#34;; exit 1; } 41 42 echo \u0026#34;üîó Attaching disk...\u0026#34; 43 qm set $TEMPLATE_ID --scsihw virtio-scsi-pci --scsi0 $STORAGE:vm-$TEMPLATE_ID-disk-0 44 45 echo \u0026#34;‚òÅÔ∏è Adding Cloud-Init drive...\u0026#34; 46 qm set $TEMPLATE_ID --ide2 $STORAGE:cloudinit 47 48 echo \u0026#34;üõ†Ô∏è Configuring boot settings...\u0026#34; 49 qm set $TEMPLATE_ID --boot c --bootdisk scsi0 50 51 echo \u0026#34;üñß Adding serial console...\u0026#34; 52 qm set $TEMPLATE_ID --serial0 socket --vga serial0 53 54 echo \u0026#34;üìå Converting VM to template...\u0026#34; 55 qm template $TEMPLATE_ID 56 57 echo \u0026#34;‚úÖ Cloud-Init Template created successfully!\u0026#34; 58 exit 0 59fi 60 61# === OPTION 2: DEPLOY VMs === 62if [[ \u0026#34;$ACTION\u0026#34; == \u0026#34;2\u0026#34; ]]; then 63 TEMPLATE_ID=$(get_input \u0026#34;Enter the template VM ID\u0026#34; \u0026#34;300\u0026#34;) 64 START_ID=$(get_input \u0026#34;Enter the starting VM ID\u0026#34; \u0026#34;301\u0026#34;) 65 NUM_VMS=$(get_input \u0026#34;Enter the number of VMs to deploy\u0026#34; \u0026#34;6\u0026#34;) 66 STORAGE=$(get_input \u0026#34;Enter the storage name\u0026#34; \u0026#34;dataz2\u0026#34;) 67 IP_PREFIX=$(get_input \u0026#34;Enter the IP prefix (e.g., 10.57.57.)\u0026#34; \u0026#34;10.57.57.\u0026#34;) 68 IP_START=$(get_input \u0026#34;Enter the starting IP last octet\u0026#34; \u0026#34;30\u0026#34;) 69 GATEWAY=$(get_input \u0026#34;Enter the gateway IP\u0026#34; \u0026#34;10.57.57.1\u0026#34;) 70 DNS_SERVERS=$(get_input \u0026#34;Enter the DNS servers (space-separated)\u0026#34; \u0026#34;8.8.8.8 1.1.1.1\u0026#34;) 71 DOMAIN_SEARCH=$(get_input \u0026#34;Enter the search domain\u0026#34; \u0026#34;merox.dev\u0026#34;) 72 DISK_SIZE=$(get_input \u0026#34;Enter the disk size (e.g., 100G)\u0026#34; \u0026#34;100G\u0026#34;) 73 RAM_SIZE=$(get_input \u0026#34;Enter the RAM size in MB\u0026#34; \u0026#34;16384\u0026#34;) 74 CPU_CORES=$(get_input \u0026#34;Enter the number of CPU cores\u0026#34; \u0026#34;4\u0026#34;) 75 CPU_SOCKETS=$(get_input \u0026#34;Enter the number of CPU sockets\u0026#34; \u0026#34;4\u0026#34;) 76 SSH_KEY_PATH=$(get_input \u0026#34;Enter the SSH public key file path\u0026#34; \u0026#34;/root/.ssh/id_rsa.pub\u0026#34;) 77 78 if [[ ! -f \u0026#34;$SSH_KEY_PATH\u0026#34; ]]; then 79 echo \u0026#34;‚ùå Error: SSH key file not found at $SSH_KEY_PATH\u0026#34; 80 exit 1 81 fi 82 83 for i in $(seq 0 $((NUM_VMS - 1))); do 84 VM_ID=$((START_ID + i)) 85 IP=\u0026#34;$IP_PREFIX$((IP_START + i))/24\u0026#34; 86 VM_NAME=$(get_input \u0026#34;Enter the name for VM $VM_ID\u0026#34; \u0026#34;ubuntu-vm-$((i+1))\u0026#34;) 87 88 echo \u0026#34;üîπ Creating VM: $VM_ID (Name: $VM_NAME, IP: $IP)\u0026#34; 89 90 if qm status $VM_ID \u0026amp;\u0026gt;/dev/null; then 91 echo \u0026#34;‚ö†Ô∏è VM $VM_ID already exists, removing...\u0026#34; 92 qm stop $VM_ID \u0026amp;\u0026gt;/dev/null 93 qm destroy $VM_ID 94 fi 95 96 if ! qm clone $TEMPLATE_ID $VM_ID --full --name $VM_NAME --storage $STORAGE; then 97 echo \u0026#34;‚ùå Failed to clone VM $VM_ID, skipping...\u0026#34; 98 continue 99 fi 100 101 qm set $VM_ID --memory $RAM_SIZE \\ 102 --cores $CPU_CORES \\ 103 --sockets $CPU_SOCKETS \\ 104 --cpu host \\ 105 --serial0 socket \\ 106 --vga serial0 \\ 107 --ipconfig0 ip=$IP,gw=$GATEWAY \\ 108 --nameserver \u0026#34;$DNS_SERVERS\u0026#34; \\ 109 --searchdomain \u0026#34;$DOMAIN_SEARCH\u0026#34; \\ 110 --sshkey \u0026#34;$SSH_KEY_PATH\u0026#34; 111 112 qm set $VM_ID --delete ide2 || true 113 qm set $VM_ID --ide2 $STORAGE:cloudinit,media=cdrom 114 qm cloudinit update $VM_ID 115 116 echo \u0026#34;üîÑ Resizing disk to $DISK_SIZE...\u0026#34; 117 qm resize $VM_ID scsi0 +$DISK_SIZE 118 119 qm start $VM_ID 120 echo \u0026#34;‚úÖ VM $VM_ID ($VM_NAME) created and started!\u0026#34; 121 done 122 exit 0 123fi 124 125# === OPTION 3: DESTROY VMs === 126if [[ \u0026#34;$ACTION\u0026#34; == \u0026#34;3\u0026#34; ]]; then 127 START_ID=$(get_input \u0026#34;Enter the starting VM ID to delete\u0026#34; \u0026#34;301\u0026#34;) 128 NUM_VMS=$(get_input \u0026#34;Enter the number of VMs to delete\u0026#34; \u0026#34;6\u0026#34;) 129 130 echo \u0026#34;‚ö†Ô∏è Destroying VMs from $START_ID to $((START_ID + NUM_VMS - 1))...\u0026#34; 131 for i in $(seq 0 $((NUM_VMS - 1))); do 132 VM_ID=$((START_ID + i)) 133 134 if qm status $VM_ID \u0026amp;\u0026gt;/dev/null; then 135 echo \u0026#34;üõë Stopping and destroying VM $VM_ID...\u0026#34; 136 qm stop $VM_ID \u0026amp;\u0026gt;/dev/null 137 qm destroy $VM_ID 138 else 139 echo \u0026#34;‚ÑπÔ∏è VM $VM_ID does not exist. Skipping...\u0026#34; 140 fi 141 done 142 echo \u0026#34;‚úÖ Specified VMs have been destroyed.\u0026#34; 143 exit 0 144fi Verifying Your Deployment # After running the script under Option 2, you should see your new VMs listed in the Proxmox web interface. You can now log in via SSH from the machine that holds the corresponding private key\nssh ubuntu@k3s-master-01 Note: Adjust the hostname or IP as configured during the script prompts.\nChapter 3: Installing K3s with Ansible # This chapter will guide you through setting up K3s using Ansible on your Proxmox-based VMs. Ansible helps automate the process across multiple nodes, making the deployment faster and more reliable.\nPrerequisites # Ensure Ansible is installed on your management machine (Debian/Ubuntu or macOS):\nDebian/Ubuntu: sudo apt update \u0026amp;\u0026amp; sudo apt install -y ansible macOS: brew install ansible Clone the k3s-ansible repository\nWe will use Techno Tim‚Äôs k3s-ansible repository, but in this guide, we‚Äôll use a forked version:\ngit clone https://github.com/mer0x/k3s-ansible Pre-Deployment Configuration # Set up the Ansible environment: 1 cd k3s-ansible 2 cp ansible.example.cfg ansible.cfg 3 ansible-galaxy install -r ./collections/requirements.yml 4 cp -R inventory/sample inventory/my-cluster\nEdit inventory/my-cluster/hosts.ini\nModify this file to match your cluster‚Äôs IP addresses. Example: 1 [master] 2 10.57.57.30 3 10.57.57.31 4 10.57.57.32 5 6 [node] 7 10.57.57.33 8 10.57.57.34 9 10.57.57.35 10 11 [k3s_cluster:children] 12 master 13 node\nEdit inventory/my-cluster/group_vars/all.yml\nSome critical fields to modify:\nansible_user: # Default VM user is ubuntu with sudo privileges. system_timezone: # Set to your local timezone (e.g., Europe/Bucharest). Networking (Calico vs. Flannel): # Comment out #flannel_iface: eth0 and use calico_iface: \u0026quot;eth0\u0026quot; for better network policies. Flannel is the simpler alternative if you prefer an easier setup. apiserver_endpoint: # 10.57.57.100 Ensure this is an unused IP in your local network. It serves as the VIP (Virtual IP) for the k3s control plane. k3s_token: # Use any alphanumeric string. metal_lb_ip_range: # 10.57.57.80-10.57.57.90 The IP belongs to your local network (LAN) It\u0026rsquo;s not already in use by other network services It\u0026rsquo;s outside your DHCP pool range to avoid conflicts This setup enables exposing K3s container services to your network, similar to how Docker ports are exposed to their host IP. Before running the next command, ensure SSH key authentication is set up between your management machine and all deployed VMs. Deploy the Cluster # Run the following command to deploy the cluster:\nansible-playbook ./site.yml -i ./inventory/my-cluster/hosts.ini Once the playbook execution completes, you can verify the cluster‚Äôs status:\n1# Copy the kubeconfig file from the first master node 2scp ubuntu@10.57.57.30:~/.kube/config . 3 4# Move it to the correct location 5mkdir -p ~/.kube 6mv config ~/.kube/ 7 8# Check if the cluster nodes are properly registered 9kubectl get nodes If the setup was successful, kubectl get nodes should display the cluster‚Äôs nodes and their statuses.\nWhat‚Äôs Next? # With K3s successfully deployed, the next steps involve setting up additional tools such as Rancher, Traefik, and Longhorn for cluster management, ingress control, and persistent storage.\nChapter 4: K3S Apps Deployment # Deploying Traefik # Install Helm Package Manager for Kubernetes # 1curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 2chmod 700 get_helm.sh 3./get_helm.sh Create Namespace for Traefik # kubectl create namespace traefik Add Helm Repository and Update # helm repo add traefik https://helm.traefik.io/traefik helm repo update Clone TechnoTim Launchpad Repository # git clone https://github.com/techno-tim/launchpad Configure values.yaml for Traefik # Open the launchpad/kubernetes/traefik-cert-manager/ directory and check values.yaml. Most configurations are already set; you only need to specify the IP for the LoadBalancer service. Choose an IP from the MetalLB range defined in your setup here.\nInstall Traefik Using Helm # helm install --namespace=traefik traefik traefik/traefik --values=values.yaml Verify Deployment # kubectl get svc --all-namespaces -o wide Expected output:\n1NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR 2calico-system calico-typha ClusterIP 10.43.80.131 \u0026lt;none\u0026gt; 5473/TCP 2d20h k8s-app=calico-typha 3traefik traefik LoadBalancer 10.43.185.67 10.57.57.80 80:32195/TCP,443:31598/TCP,443:31598/UDP 53s app.kubernetes.io/instance=traefik,app.kubernetes.io/name=traefik Apply Middleware # kubectl apply -f default-headers.yaml kubectl get middleware Expected output:\nNAME AGE default-headers 4s Deploying Traefik Dashboard # Install htpasswd # sudo apt-get update sudo apt-get install apache2-utils Generate a Base64-Encoded Credential # htpasswd -nb merox password | openssl base64 Copy the generated password hash and replace abc123== with it in dashboard/secret-dashboard.yaml 1--- 2apiVersion: v1 3kind: Secret 4metadata: 5 name: traefik-dashboard-auth 6 namespace: traefik 7type: Opaque 8data: 9 users: abc123== Apply secret\nkubectl apply -f secret-dashboard.yaml Configure DNS Resolver # Ensure that your DNS server points to the MetalLB IP specified in values.yaml here.\nExample entry for pfSense DNS Resolver:\ndashboard/ingress.yaml\nroutes: - match: Host(`traefik.k3s.your.domain`) Apply Kubernetes Resources # from traefik/dashboard folder\n1kubectl apply -f secret-dashboard.yaml 2kubectl get secrets --namespace traefik 3kubectl apply -f middleware.yaml 4kubectl apply -f ingress.yaml At this point, you should be able to access the DNS entry you created. However, it will use a self-signed SSL certificate generated by Traefik. In the next steps, we will configure Let\u0026rsquo;s Encrypt certificates using Cloudflare as the provider.\nDeploying Cert-Manager # traefik-cert-manager/cert-manager folder\nAdd Jetstack Helm Repository # helm repo add jetstack https://charts.jetstack.io helm repo update Create Namespace for Cert-Manager # kubectl create namespace cert-manager Apply CRDs (Custom Resource Definitions) # Note: Ensure you use the latest version of Cert-Manager.\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.0/cert-manager.crds.yaml Install Cert-Manager Using Helm # helm install cert-manager jetstack/cert-manager --namespace cert-manager --values=values.yaml --version v1.17.0 Apply Cloudflare API Secret # Make sure you generate the correct API token if using Cloudflare (use an API Token, not a global key).\nkubectl apply -f issuers/secret-cf-token.yaml Deploy Production Certificates # Fields to be edited before:\nissuers/letsencrypt-production.yaml\nemail, dnsZones certificates/production/your-domain-com.yaml\nname, secretName, commonName, dnsNames kubectl apply -f values.yaml kubectl apply -f issuers/letsencrypt-production.yaml kubectl apply -f certificates/production/your-domain-com.yaml Verify Logs and Challenges # kubectl logs -n cert-manager -f cert-manager-(your-instance-name) kubectl get challenges With these steps completed, your K3s cluster now runs Traefik as an ingress controller, supports HTTPS with Let\u0026rsquo;s Encrypt, and manages certificates automatically. This setup ensures secure traffic routing and efficient load balancing for your Kubernetes applications.\n‚ú® Nailed it!\nDeploying Rancher # Add Rancher Helm Repository and Create Namespace # helm repo add rancher-latest https://releases.rancher.com/server-charts/stable kubectl create namespace cattle-system Since Traefik is already deployed, Rancher will utilize it for ingress. Deploy Rancher with Helm:\n1helm install rancher rancher-stable/rancher \\ 2 --namespace cattle-system \\ 3 --set hostname=rancher.k3s.your.domain \\ 4 --set tls=external \\ 5 --set replicas=3 Create Ingress for Rancher # Create an ingress.yml file with the following configuration:\n1apiVersion: traefik.io/v1alpha1 2kind: IngressRoute 3metadata: 4 name: rancher 5 namespace: cattle-system 6spec: 7 entryPoints: 8 - websecure 9 routes: 10 - match: Host(`rancher.k3s.your.domain`) 11 kind: Rule 12 services: 13 - name: rancher 14 port: 443 15 middlewares: 16 - name: default-headers 17 tls: 18 secretName: k3s-your-domain-tls Apply the ingress configuration:\nkubectl apply -f ingress.yml Now, you should be able to manage your cluster from https://rancher.k3s.your.domain.\nDeploying Longhorn # If you want to use cloud-ready drive shared storage, follow these steps:\nInstall Required Packages # only on the VMs you want to deploy longhorn\nsudo apt update \u0026amp;\u0026amp; sudo apt install -y open-iscsi nfs-common Enable iSCSI # sudo systemctl enable iscsid sudo systemctl start iscsid Add Longhorn Label on Nodes # A minimum of three nodes are required for High Availability. In this setup, we will use three worker nodes: 1kubectl label node k3s-worker-1 storage.longhorn.io/node=true 2kubectl label node k3s-worker-2 storage.longhorn.io/node=true 3kubectl label node k3s-worker-3 storage.longhorn.io/node=true\nDeploy Longhorn # modified to use storage.longhorn.io/node=true label\nkubectl apply -f https://raw.githubusercontent.com/mer0x/merox.docs/refs/heads/master/K3S/cluster-deployment/longhorn.yaml Verify Deployment # kubectl get pods --namespace longhorn-system --watch Print Confirmation # kubectl get nodes kubectl get svc -n longhorn-system Exposing Longhorn with Traefik # Create Middleware Configuration # Create a middleware.yml file: 1apiVersion: traefik.io/v1alpha1 2kind: Middleware 3metadata: 4 name: longhorn-headers 5 namespace: longhorn-system 6spec: 7 headers: 8 customRequestHeaders: 9 X-Forwarded-Proto: \u0026#34;https\u0026#34;\nSetup Ingress # Create an ingress.yml file: 1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: longhorn-ingress 5 namespace: longhorn-system 6 annotations: 7 traefik.ingress.kubernetes.io/router.entrypoints: websecure 8 traefik.ingress.kubernetes.io/router.tls: \u0026#34;true\u0026#34; 9 traefik.ingress.kubernetes.io/router.middlewares: longhorn-system-longhorn-headers@kubernetescrd 10spec: 11 rules: 12 - host: storage.k3s.your.domain 13 http: 14 paths: 15 - path: / 16 pathType: Prefix 17 backend: 18 service: 19 name: longhorn-frontend 20 port: 21 number: 80 22 tls: 23 - hosts: 24 - storage.k3s.your.domain 25 secretName: k3s-your-domain-tls\nUsing NFS Storage # If you want to use NFS storage in your cluster, follow this guide: Merox Docs - NFS Storage Guide\nMonitoring Your Cluster # A great monitoring tool for your cluster is Netdata.\nYou can also try deploying Prometheus and Grafana from Rancher. However, if you don‚Äôt fine-tune the setup, you might notice a high resource usage due to the large number of queries processed by Prometheus.\nContinuous Deployment with ArgoCD # ArgoCD is an excellent tool for continuous deployment. You can find more details here.\nUpgrading Your Cluster # If you need to upgrade your cluster, I put some notes here: How to Upgrade K3s.\nFinal Thoughts # When I first deployed a K3s/RKE2 cluster (about a year ago), I struggled to find a single source of documentation that covered everything needed for at least a homelab, if not even for production use. Unfortunately, I couldn\u0026rsquo;t find anything comprehensive, so I decided to write this article to consolidate all the necessary information in one place.\nIf this guide helped you and you‚Äôd like to see more information added, please leave a comment, and I will do my best to update this post.\nHow Have You Deployed Your Clusters? # Let me know in the comments!\nSpecial Thanks # TechnoTim James Turland ","date":"11 February 2025","externalUrl":null,"permalink":"/blog/k3s-cluster-in-2025/","section":"Blog","summary":"Discover how I rebuilt my K3s cluster using Ansible, optimized my Kubernetes setup, and automated deployments for a more efficient and scalable homelab.","title":"How to Set Up a K3S Cluster in 2025","type":"blog"},{"content":"","date":"11 February 2025","externalUrl":null,"permalink":"/tags/kubernetes/","section":"","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"11 February 2025","externalUrl":null,"permalink":"/tags/self-hosting/","section":"","summary":"","title":"Self-Hosting","type":"tags"},{"content":"","date":"17 December 2024","externalUrl":null,"permalink":"/tags/ansible/","section":"","summary":"","title":"Ansible","type":"tags"},{"content":"Managing a homelab can feel like navigating a maze of configurations, updates, and potential errors. But what if you could rebuild your entire setup in minutes? This blog introduces a solution that not only streamlines your homelab into an automated environment but also provides a learning experience, helping you better understand server management, automation, and networking.\nReal Scenarios # Here are some practical examples of what you can achieve with this setup:\nJellyfin Streaming: Imagine accessing your entire media library securely via Jellyfin, pre-configured with SSL, directly accessible from anywhere. Proxmox Management: Manage your Proxmox dashboard securely using a custom domain like proxmox.yourdomain.com, with SSL automatically handled by Traefik. Docker Management: Quickly deploy and manage all your containers in Portainer, accessible at portainer.yourdomain.com. Requirements # This project will deploy Ubuntu Virtual Machine packed with commonly self-hosted tools like Docker, Portainer, Media Stack, Traefik, and more. Here\u0026rsquo;s what you\u0026rsquo;ll need:\nProxmox VE: For creating and managing the VM. Fresh LXC: Required to run the script mentioned below. DNS Server: A properly configured DNS setup for service domains. Essentials: A few beers to keep the spirits high! üçª This guide is designed to document my homelab automation process, but it\u0026rsquo;s flexible enough to help anyone looking to simplify their IT environment. Let‚Äôs get started! Resources # A high-level diagram of what we will implement in this blog Source code mer0x/homelab-as-code Deployment guide and infrastructure setup for automating the creation of a homelab environment using code. This repository provides the necessary scripts and configuration files to help you quickly deploy your own homelab infrastructure using tools like Ansible, Terraform, and Docker. HCL 16 3 What\u0026rsquo;s Inside the Repository? # This repository simplifies homelab deployment with everything needed to set up an VM and associated services.\nInteractive Deployment Script:\nInstalls required packages (git, curl, ansible, packer, terraform). Clones the repository (supports both public and private setups). Runs Packer, Terraform and Ansible for fully automated deployment. Configuration Folders:\nansible: Includes roles for deploying Docker, Portainer, Traefik, and optional NFS mounts. configs/docker: Pre-configured services like getHomepage, Traefik, and media stack. packer: Builds an optimized Ubuntu template for Proxmox, ready for Terraform deployments. terraform: Automates VM creation with networking and storage configurations based on packer template. Quick start # Execute the following command:\nbash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/mer0x/homelab-as-code/refs/heads/master/deploy_homelab.sh)\u0026#34; Tip: Run this script on a clean LXC for best results.\nInitial Prompts\nFirst, the script will ask for the repository type (public or private). For a private repository, ensure you provide the SSH link (e.g., git@github.com:mer0x/homelab.git) instead of HTTPS. For a public repository, you can directly use this one tutorial-based: https://github.com/mer0x/homelab-as-code.git If private is selected, the script generates an SSH key and prompts you to add the public key to your repository under Settings ‚Üí Deploy Keys (e.g., in GitHub). Confirmation Prompt\nNext, the script asks if you‚Äôve edited the necessary files for deployment. Type yes to proceed with the Packer, Terraform and Ansible automation. If you type no, review the blog for guidance on editing the required files, then re-run the script after making changes. Resources: You will find the downloaded configuration in /home/homelab/ Ansible # Inventory Folder # The inventory folder is essential for Terraform and Ansible integration, as it holds the dynamically generated inventory.ini file. This file maps the VM IP address and SSH credentials for Ansible to use.\nHere‚Äôs how it‚Äôs created in the Terraform configuration:\nresource \u0026#34;local_file\u0026#34; \u0026#34;ansible_inventory\u0026#34; { depends_on = [time_sleep.wait_1_minute] content = \u0026lt;\u0026lt;EOT [docker] docker-01 ansible_host=IP_DEPLOYMENT ansible_user=root ansible_ssh_private_key_file=~/.ssh/id_rsa EOT filename = \u0026#34;../ansible/inventory/inventory.ini\u0026#34; } You don\u0026rsquo;t need to edit anything in this folder manually, as it is automatically generated during deployment.\nRoles Folder # The roles/docker folder contains all the necessary playbooks for setting up:\nDocker Docker Compose Portainer getHomepage Traefik Each service has a specific .yml file that automates its installation and configuration.\nConfiguring NFS Mount (Optional) # If you plan to use an external NFS share, update the roles/nfs_mount file with your specific configuration:\nStep 1: Replace the placeholder with your NFS server\u0026rsquo;s IP or FQDN: REPLACE_this_with_your_nfs_server_ip_address Step 2: Update the path to the shared directory on your NFS server: REPLACE_this_with_your_nfs_path In the end, your line should look like this:\n172.20.0.254:/volume1/Server/Data/media_nas/ /media nfs rw,hard,intr 0 0 Note: If you decide not to use an NFS share, click here to follow the steps for disabling this feature in your deployment.\nConfigs # This section focuses on the second folder in the repository, which contains configurations for:\ngetHomepage Media Stack Traefik getHomepage # /docker/homepage/ # No edits are needed in the homepage folder. If you\u0026rsquo;d like to use an existing configuration, you can extract the archive, add your files, and re-archive it. Alternatively, after deployment, configurations will be available in the VM under /home/homepage/.\nMedia Stack Configuration # /docker/media-stack/ # To ensure the Media Stack services are accessible within your network, update the DNS values in the labels section of the media_stack configuration. Focus on the \u0026quot;rule=Host\u0026quot; entries, where you need to replace the example DNS values with your own domain or DNS configuration from your local DNS server (e.g., Pi-hole, Unbound, Technitium).\nFor example:\n- \u0026#34;traefik.http.routers.jellyseer-media.rule=Host(`jellyseer.local`)\u0026#34; Replace jellyseer.local with your custom domain:\n- \u0026#34;traefik.http.routers.jellyseer-media.rule=Host(`jellyseer.merox.cloud`)\u0026#34; This ensures the services are accessible at jellyseer.merox.cloud or any domain configured in your environment.\nTraefik # V1: Return to this step after deployment is complete (config will be in /home/traefik on the deployed machine).\nV2: Extract the archive, edit as shown below, and re-archive.\ndocker-compose.yaml # Update all instances of yourdomain.com to your actual domain. Ensure this change is made in all four occurrences.\nProvider used: Cloudflare.\n.env # This hidden file contains credentials for the Traefik dashboard. Default credentials:\nUsername: user Password: password Edit these if needed.\ndata/config.yml # Optional file for adding services outside Docker (e.g., your Proxmox IP for SSL access at proxmox.mydomain.com).\ndata/traefik.yml # Replace your-cloudflare@email.com with your Cloudflare email.\nPost-Deployment Step # After deployment, create a file named cf_api_token.txt in /home/traefik/ with your Cloudflare API token. This step is crucial for starting the Traefik container. Start the container manually:\n/usr/local/bin/docker-compose up \u0026amp; Additional Resources # For more on Traefik, check out this detailed guide.\nPacker # credentials.pkr.hcl # This file holds sensitive credentials like passwords and API tokens. Do not upload this file to public repositories.\nproxmox_api_url = \u0026#34;https://your-proxmox-ip-or-fqdn/api2/json\u0026#34; proxmox_api_token_id = \u0026#34;terraform_user@pam!homelab\u0026#34; proxmox_api_token_secret = \u0026#34;your-proxmox-api-token-secret\u0026#34; To generate your Proxmox API token, follow this tutorial.\nMore about packer: here.\npacker.pkr.hcl # This file contains the plugin configuration required for deployment on Proxmox.\npacker { required_plugins { proxmox-iso = { version = \u0026#34;\u0026gt;= 1.0.0\u0026#34; source = \u0026#34;github.com/hashicorp/proxmox\u0026#34; } } } ubuntu-*folder*/ubuntu-server-jammy-docker.pkr.hcl # Replace YOUR_PROXMOX_NODE_NAME with the name of your Proxmox node where you want the template to be created. Replace YOUR_IP_DEPLOYMENT_MACHINE with the IP of the machine where the deployment script will run. ubuntu-*folder*/http/user-data # Replace YOUR_SSH_KEY with the SSH key generated by the homelab_deploy.sh script. Terraform # terraform.tfvars # This file holds sensitive credentials like passwords and API tokens. Do not upload this file to public repositories.\nExample:\nproxmox_api_url = \u0026#34;https://your-proxmox-ip-or-fqdn/api2/json\u0026#34; proxmox_api_token_id = \u0026#34;terraform_user@pam!homelab\u0026#34; proxmox_api_token_secret = \u0026#34;your-proxmox-api-token-secret\u0026#34; proxmox_host = \u0026#34;proxmox-host-ip\u0026#34; proxmox_user = \u0026#34;proxmox-user\u0026#34; proxmox_password = \u0026#34;proxmox-password\u0026#34; To generate your Proxmox API token, follow this tutorial.\nmodules/docker_vm/main.tf # Defines the deployment\u0026rsquo;s detailed configuration.\nReplace the placeholders with your specific values: YOUR_PROXMOX_NODE_NAME IP_MACHINE ( VM deployment IP ) GateWay_IP IP_DEPLOYMENT ( same-as-IP_MACHINE ) YOUR_SSH_KEY ( generated by the homelab_deploy.sh script ) Key features enabled:\nNFS/CIFS Mounting: Supports external storage mounts. # Note: If you don‚Äôt want to use any NFS mount, simply comment out this block:\nresource \u0026#34;null_resource\u0026#34; \u0026#34;run_ansible_docker\u0026#34; { depends_on = [local_file.ansible_inventory] provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;LC_ALL=C.UTF-8 LANG=C.UTF-8 ansible-playbook -i ../ansible/inventory/inventory.ini ../ansible/roles/nfs_mount/main.yml\u0026#34; } } Locales Configuration: Sets up default locales. Root SSH Login: Allows root access via SSH. SSH Key Authentication: Adds your SSH public key for secure access. Ansible Integration: Automates further configurations. Post Deployment # A successful deployment will show the following output in your console:\nYou can now access your Docker environment via Portainer using the IP (IP_MACHINE) and port 9000. Example:\nhttp://172.20.0.252:9000 After setting up your Portainer user, you should see a dashboard like this:\n‚ö†Ô∏è Best Practice: Regularly check your automation, at least monthly, to ensure everything is functioning correctly.\nIf you encounter issues, leave a comment, and I‚Äôll assist as soon as possible.\nConclusion # Thank you for exploring this Homelab as Code project! By combining Packer, Terraform and Ansible, we‚Äôve streamlined homelab automation and setup. I hope this guide inspires you to enhance your IT skills and optimize your infrastructure.\nHappy homelabing, and don\u0026rsquo;t hesitate to share your journey or questions‚Äîyour feedback is always appreciated! üöÄ\nFuture plan # In the coming weeks, I plan to expand this project by adding support for VM deployment using Cloud-Init and Packer. Additionally, I‚Äôm considering integrating advanced Ansible projects, such as K3S from TechnoTim, to enhance the automation and scalability of homelab environments.\n","date":"17 December 2024","externalUrl":null,"permalink":"/blog/homelab-as-code/","section":"Blog","summary":"Explore the evolution of a homelab, from its humble beginnings to a fully automated infrastructure. Learn how to implement cutting-edge tools and techniques for IT experimentation and skill-building.","title":"Homelab as Code: Packer + Terraform + Ansible","type":"blog"},{"content":"","date":"17 December 2024","externalUrl":null,"permalink":"/tags/terraform/","section":"","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"5 November 2024","externalUrl":null,"permalink":"/tags/case-study/","section":"","summary":"","title":"Case-Study","type":"tags"},{"content":" Why a Homelab? # During my early days at work, I became fascinated by how remote locations seamlessly communicated with the central hub. This sparked my desire to create my own accessible work environment‚Äîa homelab. Building my homelab from scratch allowed me to experiment, learn, and develop skills that proved invaluable in my IT career. I started with a simple Raspberry Pi, which allowed me to access a Linux device remotely and continue learning and experimenting. At work, we primarily used site-to-site IPsec VPNs (via StrongSwan) to interconnect multiple locations, along with client-to-site OpenVPN servers for home office access. While the IPsec setup felt complex at the time, I opted to start with a more manageable client-to-site OpenVPN configuration for my homelab.\nMy First Homelab Setup # With OpenVPN, I could remotely access my homelab from anywhere. This setup became my personal playground, where I explored theories, tested configurations, and refined skills I knew would be essential in my career.\nFirst Homelab Setup - Raspberry Pi Starting with Raspberry Pi and OpenVPN # Setting up my first homelab with Raspberry Pi was an experience filled with trial and error. Initially, I struggled to detect the Raspberry Pi on my network because it was assigned a DHCP IP address rather than a static one. With limited networking experience at the time, I resorted to analyzing MAC addresses through the router and gradually ruled out devices one by one until I identified the assigned IP‚Äîan effective but time-consuming approach in a small network.\nConfiguring OpenVPN presented its own set of challenges. Generating certificates for the client and configuring OpenVPN for optimal remote access required testing many parameters. I experimented with different settings to ensure that I could securely connect from anywhere, which involved numerous trial runs.\nThese hours spent troubleshooting taught me the value of reading documentation thoroughly and exploring diverse perspectives from other users on platforms like Stack Overflow, GitHub Issues, and the OpenVPN forums. This process of researching solutions and piecing together knowledge from various sources laid a solid foundation for my troubleshooting skills.\nTop 3 VPNs Tested in my Homelab # VPN Protocol Performance Configuration Security Main Drawback OpenVPN Medium Complex High Slower than others WireGuard High Simple High Limited flexibility Tailscale High Very Simple Moderate Third-party managed Throughout the evolution of my homelab, I‚Äôve tested several VPN solutions ( 2 of them can be #selfhosted ) to ensure a secure remote connection. Here‚Äôs a comparison of the top three VPNs I used, including my current preference and an additional modern option:\nOpenVPN\nPros: It‚Äôs open-source, highly configurable, and widely supported across different platforms, making it a solid choice for homelabs. Cons: However, OpenVPN tends to have a more complex configuration process and slightly lower performance compared to newer VPN protocols. Experience: Although OpenVPN was challenging to set up initially, it taught me valuable configuration skills and served as my primary VPN solution for quite a while. WireGuard\nPros: WireGuard is lightweight, offering faster connection speeds and lower latency, ideal for homelabs where quick access is essential. Its lean codebase also reduces potential security vulnerabilities. Cons: It‚Äôs less flexible in terms of configuration options compared to OpenVPN, and it may not yet be supported on all legacy systems. Experience: In my setup, WireGuard offered a noticeable improvement in performance, with a simplified deployment process and better stability, which is why it remains my preferred choice for securing homelab access. Tailscale (3rd-Party Managed Option)\nPros: The major advantage of Tailscale is its ease of deployment‚Äîwithin minutes, you can have a secure network set up between devices. It also bypasses many firewall and NAT configurations, simplifying remote access even further. Cons: The downside is its reliance on a third-party service for managing the VPN connections, which might not align with privacy goals or enterprise homelab setups. For those wanting full control, Tailscale‚Äôs third-party dependency could be a drawback. Experience: While Tailscale‚Äôs quick deployment is impressive, especially for fast access and smaller setups, I personally lean towards WireGuard for its control and privacy advantages in a homelab environment. Accessing my homelab securely sparked ideas and inspired experiments, allowing me to explore various topics and scenarios. I soon discovered that transitioning to an IT infrastructure was not as distant as I had initially thought. Fast forward 4 to 5 years later, and that vision of an infrastructure has begun to materialize.\nPrimary Purposes for VPNs Security Tip: Understanding Scripts # If you choose to use a script for deployment, always review it carefully. Understanding each part is crucial to avoid compromising the security of the system on which it runs. Lessons Learned from Building My First Homelab # One of the most valuable lessons was the importance of doing everything manually and troubleshooting each error myself rather than relying on ready-to-go scripts available online. This approach not only strengthened my homelab but also helped me develop out-of-the-box thinking, an invaluable skill in my professional IT career. The patience and persistence required for troubleshooting built a solid foundation, preparing me for more complex projects as my homelab evolved.\nConclusion and Further Reading # Creating a homelab has been a pivotal experience in my IT journey. If you are interested in building your own or are simply curious about the process, stay tuned for the next posts in this series. We‚Äôll explore how my setup evolved, covering hardware upgrades, virtualization, and system optimization to create an efficient and powerful homelab environment.\nFor those interested in my beginnings in the IT industry, feel free to check out the first steps in my IT career here. You can also see the technical specifications of my current homelab setup in more detail on my homelab infrastructure documentation page.\n","date":"5 November 2024","externalUrl":null,"permalink":"/blog/homelab-evolution/","section":"Blog","summary":"A thorough examination of how a homelab setup has changed over time, from early builds to the infrastructure that exists now. Discover how practical experimenting in a home lab can improve IT abilities.","title":"Laboratory Evolution: How It All Started","type":"blog"},{"content":"","date":"5 November 2024","externalUrl":null,"permalink":"/categories/networking/","section":"","summary":"","title":"Networking","type":"categories"},{"content":"","date":"5 November 2024","externalUrl":null,"permalink":"/tags/vpn/","section":"","summary":"","title":"Vpn","type":"tags"},{"content":"","date":"25 October 2024","externalUrl":null,"permalink":"/tags/guide/","section":"","summary":"","title":"Guide","type":"tags"},{"content":"","date":"25 October 2024","externalUrl":null,"permalink":"/tags/interview/","section":"","summary":"","title":"Interview","type":"tags"},{"content":" March 2018 # This marks the beginning of my journey into the IT industry. At that time, I was 20 years old and in my second year of a Computer Science program. I had never been particularly fond of learning or attending university classes, and I lacked the confidence to apply for jobs. However, it quickly became clear that sometimes it\u0026rsquo;s important not to undervalue yourself but to be at least realistic about your knowledge.\nThe Push I Needed # It likely would have taken me a long time to muster the courage to submit my first CV, as I always felt I didn\u0026rsquo;t have enough knowledge‚Äîespecially since I wasn\u0026rsquo;t attending classes. Yet, I had been interacting with servers and databases since I was around 11-12 years old, back in 2008-2009. This background is part of the IT journey that continues to fuel my curiosity even now, at 27, which I will delve into in a future blog post.\nI want to take a moment to thank @Dacian for giving me that much-needed push to get going. Dacian, a good friend from the tech scene, practically dragged me out of bed to create my CV (I was indeed indifferent, not just underestimating my skills). My first CV was created in NOTEPAD! I even went so far as to print it out in case sending it online wasn\u0026rsquo;t enough‚Äîat that time, I had no idea how job applications worked. It was the first time I had ever done something like that.\nThe Opportunity # So, after some persuasion, my friend had seen an advertisement a few days earlier for a System Administrator position at a company called Netex. His excitement was contagious because, let\u0026rsquo;s face it, most people in IT, at least in my town, associated the field strictly with programming. Knowing I was interested in sysadmin roles (having run an online gaming community with him back in 2012), he didn\u0026rsquo;t hesitate to support me and push me into the workforce üòÑ.\nThe Interview Process # When I arrived at the company\u0026rsquo;s headquarters, I felt a bit anxious, but the process was quick. I submitted my CV and was informed that someone from HR would contact me soon for an update. After leaving, I almost completely forgot about that moment, but within a week, I received a phone call to schedule an interview.\nThe Technical Interview # It had been nearly two weeks since I submitted my CV, and the day of the interview finally arrived. At the company\u0026rsquo;s main office, I was invited into a conference room with an HR representative and the head of the IT department. They started by introducing the company, what they did, and what they were looking for. Then, it was my turn to express what I sought from an employer and share my experience.\nI realized I had the logic but perhaps not the appropriate tech vocabulary. Nevertheless, this logic led to a pleasant and relaxed conversation. First IT job interview questions # Here are some of the questions and how I recall responding:\nIf you\u0026rsquo;re a beginner, try to answer these questions before checking the solutions! üòÑ\nQ: What is the administrator user in Linux? Show Answer A: root Q: How do you view the open ports locally on a Linux machine? Show Answer A: netstat -tulpen Q: How do you view open ports on a remote Linux machine? Show Answer A: nmap Q: What are the basic file permissions in Linux? Show Answer A: r - read(4), w - write(2), x - execute(1) Q: What are the ports for the following protocols?(https, ssh, imap, smtp, dns, etc.) Show Answer A: http - 80; https - 443; ssh - 22; I don\u0026rsquo;t know the others off the top of my head, but I can find out quickly using the command: cat /etc/services | grep -i mysql My experience has taught me that responses like this, which are somewhat out-of-the-box, are highly appreciated as they provide interviewers with a clear perspective about you, showing that you haven\u0026rsquo;t just memorized facts but truly understood the logic behind the topics.\nQ: What is /etc/shadow? Show Answer A: A file that contains local users and their encrypted passwords on Linux systems, visible only to the root user. The Offer # The next day, I decided to attend a class at the university since I was in Timi»ôoara, where I was enrolled (I was commuting from my hometown). During that time, I didn\u0026rsquo;t attend classes frequently as I had become quite bored with the idea of university. I do not recommend anyone to skip out, as I believe it\u0026rsquo;s a significant advantage, just not for me.\nI had barely finished my first class when I got a call from the company where I interviewed, telling me I got the job! Upon arriving at the office, I met again with HR and the department head. I accepted the offer, which I believe was slightly above the minimum wage in Romania. I was not focused on the salary aspect, as I had never approached this field with that mindset; it seemed to come naturally.\nNext Steps # From this point, my technical journey at my first job can be found here: merox.dev/resume/. It was certainly a place where I learned a lot and even made friends; I still occasionally go out for a beer with my former department head. üòä\nThis concludes the first chapter of my Merox IT Journey series that I am starting on my blog. If you made it this far, I hope you found my experience at least somewhat interesting. I welcome your feedback and would love for you to share your own experiences as well!\nStay tuned for more stories from my IT journey in future blog posts! Your browser does not support the audio element. Narration is AI generated with ElevenLabs\nExtra # Previous Nextsads some nostalgic photos from that period\n","date":"25 October 2024","externalUrl":null,"permalink":"/blog/first-steps-in-it-career/","section":"Blog","summary":"This marks the beginning of my journey into the IT industry. At that time, I was 20 years old and in my second year of a Computer Science program\u0026hellip;","title":"My First Steps in IT Career","type":"blog"},{"content":" The most ambitious and challenging project I‚Äôve ever undertaken. Image Link Details Infra Infrastructure presentation Net Networking setup and maintenance Hypervisors Managing virtualization layers VMs VMs setup and configuration Docker Container orchestration and management K8S Cluster management Ansible Automation and configuration management Security Security measures and firewall setup Mon Monitoring tools and alerts ","date":"10 October 2024","externalUrl":null,"permalink":"/laboratory/","section":"Merox.dev - IT Blog \u0026 Documentation","summary":"Homelab, the most ambitious and challenging project I‚Äôve ever undertaken","title":"Homelab","type":"page"},{"content":" Download CV \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e System Administrator 04/2018 - 06/2020 Netex Romania S.R.L., Timisoara, TM Netex Romania is a multinational company headquartered in Timisoara, providing Customer Relations services in Europe.\nManaged backup processes using BAREOS, ensuring data integrity and redundancy. Provided comprehensive training to internal and off-site users to optimize systems maintenance. Performed patching and troubleshooting on NetexOS (Debian-based system) with automation through Puppet. Maintained and migrated Puppet from bare metal infrastructure to a Kubernetes (K8s) cluster. Mounted and configured servers (HP and Dell) and CISCO switches in the datacenter. Installed XEN hypervisors and integrated Docker for the development team. Managed VOIP systems (Asterisk, CISCO phones). Network configurations on CISCO switches. Configured IPsec site-to-site connections using StrongSwan on Linux servers. Developed a Python script for rebooting CISCO phones via Telnet. Created a Bash script for CA expiry email notifications for IPsec tunnels. Cyber Security Engineer 07/2020 - 03/2022 Atos It Solutions, Timisoara, TM Atos is the global leader in secure and decarbonized digital solutions.\nConducted risk and vulnerability assessments using NESSUS, providing actionable recommendations. Advised on security system improvements, including policy rule migration for Windows 10 users using Forescout. Implemented firewall rules on CISCO, Fortinet, Palo Alto, and Juniper next-generation firewalls. Configured and managed proxy rules on McAfee and SQUID systems. Troubleshot and cleaned up firewall and proxy rules. Monitored data access and safeguarded sensitive information using CyberArk. Founder 07/2022 - 05/2024 Dolphost Webhosting and Domains\nDeveloped websites using WordPress, with a strong emphasis on security best practices and measures. Provided maintenance and sales of VPS and web hosting services, ensuring high performance and customer satisfaction. Engaged in domain reselling, managing registrations and transfers for various domains. Installed and configured technologies such as WHMCS and cPanel for managing web hosting services. More on: merox.dev/websites \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e System Administrator 03/2022 - Present Forvia, Giarmata, TM FORVIA provides solutions for safe, sustainable, and advanced mobility.\nMaintained Windows and Linux servers. Integrated ActiveDirectory and SALT minions on Linux servers. Managed virtual servers using ManageIQ. Managed network configurations with VitalQIP. Patching Windows servers with Control-M. Installed and configured PBS (Portable Batch System) on an HPC server for job scheduling and resource allocation. ","date":"10 October 2024","externalUrl":null,"permalink":"/resume/","section":"Merox.dev - IT Blog \u0026 Documentation","summary":"Explore Robert Melcher (Merox\u0026rsquo;s) professional journey in IT, showcasing extensive experience in web development, server management, and hosting services. Discover skills, projects, and milestones that highlight a dedicated and diverse career in technology.","title":"Resume","type":"page"},{"content":" In this section, I proudly present a selection of websites I have developed over the years. From personal portfolios to eCommerce platforms explore the screenshots below to see how I bring ideas to digital product. vNoted.com AI-Powered Tech Blog An intelligent tech blog where content is autonomously generated by AI. Designed to facilitate answers to tech-related topics and provide insights into recent industry news.\nRole: AI Content Automation Creator: vNoted Team Link: vNoted.com Plative.ro Traffic Communication App A sophisticated traffic communication application for Romanian drivers. Built the presentation and download platform, with a focus on security implementation.\nRole: Security Implementation App Developer: Alex Paczeika Link: plative.ro DOCS.merox.dev Documentation Website A technical documentation site built with MkDocs (Squidfunk theme) and hosted on GitHub Pages via GitHub Actions. The site provides detailed guides and information about my homelab and related IT projects.\nPlatform: MkDocs (Squidfunk Theme) Hosting: GitHub Pages Automation: GitHub Actions Status: Active AVA Delivery Pizza eCommerce Platform A fully-featured eCommerce solution developed during my time running dolphost.ro. Custom-built for a pizza delivery business with modern ordering capabilities.\nType: eCommerce Platform Features: Online Ordering, Menu Management Status: Completed Project Dolphost.ro Web Hosting Platform A comprehensive web hosting platform built on WHMCS, offering domain registration and hosting services.\nPlatform: WHMCS Services: Web Hosting, Domain Registration Status: Previously Active RobertMelcher.ro Personal Website My previous personal website showcasing my portfolio and services, built using WordPress.\nPlatform: WordPress Type: Personal Portfolio Status: Previous Version ","date":"10 October 2024","externalUrl":null,"permalink":"/websites/","section":"Merox.dev - IT Blog \u0026 Documentation","summary":"Discover Merox\u0026rsquo;s portfolio of website projects, developed with expertise in platforms like WordPress, Joomla, Hugo, and MkDocs. Each project highlights a commitment to secure, efficient, and user-centered web solutions tailored to diverse client needs.","title":"Websites Portofolio","type":"page"},{"content":"","date":"30 September 2024","externalUrl":null,"permalink":"/tags/alexa/","section":"","summary":"","title":"Alexa","type":"tags"},{"content":"","date":"30 September 2024","externalUrl":null,"permalink":"/tags/homeassistant/","section":"","summary":"","title":"Homeassistant","type":"tags"},{"content":"","date":"30 September 2024","externalUrl":null,"permalink":"/categories/smart-home/","section":"","summary":"","title":"Smart Home","type":"categories"},{"content":"What started as a simple wish to control lights using my phone quickly turned into a fully integrated smart home setup. I‚Äôve always been a bit lazy when it comes to getting up to switch things off manually, but once I discovered the potential of smart devices like Amazon Alexa and Home Assistant, I was hooked. Over time, I‚Äôve expanded the system to include a wide range of devices that automate my entire home.\nDevices I Use # In my smart home setup, I‚Äôve integrated several key devices:\nAmazon Alexa: I use Alexa in every room (living room, bedroom, kitchen) to control most devices via voice commands. Home Assistant: Running on a Dell server through a VM, Home Assistant is the central brain of my setup, managing more complex automations and scripts. Broadlink RM4 Pro: This acts as a bridge for RF and IR devices, allowing me to control roller blinds and non-smart appliances. Philips Hue \u0026amp; Goove Lights: These add mood lighting, often synchronized with specific routines like \u0026ldquo;movie time\u0026rdquo;. Roborock Vacuum, Resideo Thermostat, and air conditioning units. Smart Roller Blinds: Controlled through Broadlink using RF signals. In the near future, I plan to add cameras and a Yale smart lock.\nAlexa and Home Assistant # Initially, Alexa controlled most of the devices, but once I introduced HA(Home Assistant), things got even more flexible. I now have full automation based on my location, weather conditions, and custom scripts. For example, when I leave the house, motion sensors activate, and all roller blinds close automatically. When I return, the system opens the blinds, adjusts the temperature based on the outside weather, and even turns on night lights to guide me through the dark.\nExample Automations # Morning and Night Routines # One of the key routines I rely on is Good Morning and Good Night, which create a seamless transition from sleep to waking and vice versa. These routines handle:\nOpening the blinds to a certain level in the morning to let in natural light. Shutting down all lights and closing the blinds in the evening to create the perfect environment for sleep. This automation has made mornings feel more natural and relaxing, and night routines are much more consistent without having to remember to switch everything off.\nMovie and Gaming Time # When it‚Äôs time for a movie or gaming session, I have created automations to handle the ambiance and setup:\nMovie Time: The blinds in the living room close, the TV turns on and switches to Netflix, the soundbar is set to a specific sound mode, and the air conditioning turns on if it‚Äôs summer. Additionally, the ambient lighting around the TV adjusts to create the perfect movie-watching environment. Gaming Time: Similar to Movie Time, but the TV connects to the PlayStation 5, and the lights adjust for a more immersive experience. Alexa - Routine1 Alexa - Routine2 Philips Hue Alexa, movie time\nSecurity Setup # For security, my motion sensors automatically trigger when I leave, and I receive notifications via Alexa and Home Assistant if anything unusual happens. For instance, any significant changes in room temperature or other unexpected events will prompt alerts, helping me stay in control even when I‚Äôm away.\nTutorial: Integrating Broadlink with Alexa \u0026amp; RF Learning # Here‚Äôs a simple tutorial on how I used Broadlink RM4 Pro to integrate RF devices like my roller blinds with Alexa.\nStep 1: Set Up Broadlink RM4 Pro # Download the Broadlink app and follow the steps to connect the RM4 Pro to your Wi-Fi. Register an account and add the RM4 Pro device in the app. Step 2: Learn RF Signals # Select Add Remote in the Broadlink app, and choose \u0026ldquo;RF Appliance\u0026rdquo;. Use your original remote for the roller blinds, press the corresponding button while the app is in Learning Mode, and save the learned code. Test the new button to make sure it controls the blinds. Step 3: Connect Broadlink to Alexa # In the Alexa app, enable the Broadlink skill. Link your Broadlink account and let Alexa discover your RF devices. Now, you can use voice commands like, ‚ÄúAlexa, close the blinds,‚Äù and it will work seamlessly.\nBroadlink - Dashboard Broadlink - IR/RF Broadlink - Learning Challenges and Advanced Automations # The Most Challenging Part: Running Linux Scripts # One of the more challenging aspects was getting Home Assistant to correctly execute Linux scripts based on my GPS location. Using bash commands, I created scripts that adjust server fan speeds based on external temperatures when I leave or return home.\nFor example:\nWhen I‚Äôm away, the server fan speeds up to keep the system cool. Lights automatically adjust, and blinds close. When I come back, the system reverses these actions. /root/homeassistant/configuration.yaml #!/bin/bash # Adjust server fan speed based on external temperature shell_command: set_fans_home: \u0026#39;ssh -i /config/ssh/id_rsa -o StrictHostKeyChecking=no root@10.10.10.10 /usr/bin/ipmitool -I lanplus -H 10.10.10.200 -U root -P SuperSecretPassword raw 0x30 0x30 0x02 0xff 0x14\u0026#39; set_fans_away: \u0026#39;ssh -i /config/ssh/id_rsa -o StrictHostKeyChecking=no root@10.10.10.10 /usr/bin/ipmitool -I lanplus -H 10.10.10.200 -U root -P SuperSecretPassword raw 0x30 0x30 0x02 0xff 0x28\u0026#39; Future Plans # In the near future, I plan to integrate more advanced routines based on my location and possibly automate the Yale smart lock to engage whenever I leave home. The ability to do this with a combination of Alexa and Home Assistant makes the whole process incredibly smooth.\nMore automations # Below, you can see more simple automations videos from my smart home Balcony lights Roborock start cleaning Hue motion sensor\nConclusion # Building a smart home is an ongoing project, and with tools like Broadlink, Alexa, and Home Assistant, it‚Äôs more accessible than ever. Whether you‚Äôre just starting or already have a setup, there‚Äôs always room for improvement and more efficient automation. The beauty of smart home devices lies in how they can adapt to your personal routines and preferences, making everyday life a little easier.\nCredits # üìπ Smart Home Solver - YouTube Channel. üìπ Grayson Adams - YouTube Channel. üìπ Smart Home Junkie - YouTube Channel. üìπ Everything Smart Home - YouTube Channel. ","date":"30 September 2024","externalUrl":null,"permalink":"/blog/smarthome-journey/","section":"Blog","summary":"What started as a simple wish to control lights using my phone quickly turned into a fully integrated smart home setup\u0026hellip;","title":"Smart Home Journey","type":"blog"},{"content":"","date":"16 August 2024","externalUrl":null,"permalink":"/tags/ai/","section":"","summary":"","title":"Ai","type":"tags"},{"content":"Recently, I started exploring the field of AI to better understand what my home lab setup is capable of. This time, I used a Dell PowerEdge R720 server, equipped with 192 GB of RAM, dual 6-core CPUs (24 threads), and no GPU, to familiarize myself with various AI tools. Having done some investigation into this matter, three major tools attracted my attention-Stable Diffusion, OpenWebUI and Ollama. Each tool has unique features while together they make up a functional AI environment for me to play around with.\nBelow are brief descriptions of the tools:\nStable Diffusion: A model that generates high-quality images from text prompts, ideal for exploring AI-generated art. OpenWebUI: An intuitive interface that makes it easier to manage and interact with AI models. Ollama: A versatile AI framework designed for seamless integration and reliable performance. In this article, I will go over how I set these tools up on my server as well as their configurations. The difficulties that were there , the solutions I found and useful tips that may be helpful when working under similar conditions will also be shared. Even though still new in Artificial Intelligence systems at least my experience can be of great help to others who may want to start too.\nSetting Up Ollama: My First Step into AI # The first tool I tackled was Ollama. After doing some research, it seemed like a good starting point because of its straightforward setup and broad compatibility. Ollama is a flexible AI framework that can be installed on all major platforms‚ÄîLinux, MacOS, and Windows‚Äîwhich made it accessible no matter what system you‚Äôre running. Plus, it provides a solid foundation for further AI experiments, which is exactly what I needed for my home lab.\nInstalling Ollama on Linux # I began by setting up Ollama on a virtual machine running Ubuntu 22.04 in my Proxmox environment. I allocated 20 cores and 64GB of RAM to this VM to ensure smooth performance during testing.\nTo install Ollama, I used the following command:\ncurl -fsSL https://ollama.com/install.sh | sh Downloading the Llama3 Model # Once the installation was complete, the next step was to download a model to work with. I chose the Llama3 model, which is known for its performance in various AI tasks. To download it, I ran the command:\nollama pull llama3 Running the Llama3 Model # ollama run llama3 example \u0026ldquo;llama3 cli example\u0026rdquo; # How does the concept of time dilation in Interstellar relate to Einstein\u0026rsquo;s theory of relativity?\nThe concept of time dilation in Interstellar is directly related to Albert Einstein\u0026rsquo;s theory of special relativity, specifically the concept of time dilation. Here\u0026rsquo;s how:\nIn the movie, a wormhole allows Cooper\u0026rsquo;s spaceship to travel at incredibly high speeds, approaching relativistic velocities. As they approach the speed of light (0.99c), time appears to slow down for them relative to Earth. This is precisely what Einstein predicted in his theory\u0026hellip;\u0026quot;\nSetting Up Stable Diffusion # Stable Diffusion is a powerful model for generating high-quality images from text prompts. Setting it up on my Ubuntu 22.04 VM was quite straightforward thanks to the clear installation guide provided in the official GitHub repository.\nInstallation Steps # Here‚Äôs a summary of the steps I followed to install Stable Diffusion:\nInstall the Dependencies # Depending on your Linux distribution, use one of the following commands to install the necessary dependencies:\nDebian-based (e.g., Ubuntu):\nsudo apt install wget git python3 python3-venv libgl1 libglib2.0-0 Red Hat-based:\nsudo dnf install wget git python3 gperftools-libs libglvnd-glx openSUSE-based:\nsudo zypper install wget git python3 libtcmalloc4 libglvnd Download the Web UI Script # Navigate to the directory where you want to install Stable Diffusion and execute the following command:\nwget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh Run the WebUI Script # ./webui.sh Configure the webui-user.sh File # Since my server does not have a GPU, I needed to adapt the webui-user.sh file to optimize performance for CPU usage. Specifically, I added the following line to configure the environment for CPU-only operation:\nexport COMMANDLINE_ARGS=\u0026#34;--lowvram --precision full --no-half --skip-torch-cuda-test\u0026#34; \u0026ndash;lowvram: Reduces memory usage to accommodate systems with limited GPU memory.\n\u0026ndash;precision full: Ensures full precision calculations, useful for CPU processing.\n\u0026ndash;no-half: Disables half-precision calculations to avoid potential issues on CPUs.\n\u0026ndash;skip-torch-cuda-test: Skips tests related to CUDA, which is irrelevant on systems without GPUs.\nStart the Web UI # By default, running the webui.sh command will start the server and bind it to localhost (127.0.0.1). To allow access from other interfaces (useful for integration with OpenWebUI), use the \u0026ndash;listen parameter:\n./webui.sh --listen This command makes the server accessible on all network interfaces at port 7860 (0.0.0.0:7860).\nStable Diffusion Webpage Setting Up OpenWebUI # With Ollama and Stable Diffusion installed and configured, the next step is to centralize these tools into a single, user-friendly interface. OpenWebUI provides a cohesive platform to interact with your AI models, similar to ChatGPT.\nInstallation Steps # Setting up OpenWebUI is probably the easiest and quickest part of this tutorial, thanks to its well-organized and straightforward documentation. You can deploy OpenWebUI using Docker, which simplifies the installation process.\nInstallation with Default Configuration # If Ollama is running on the same server as OpenWebUI, use the following Docker command:\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main If Ollama is hosted on a different server, you need to specify the URL of the Ollama server:\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main For running OpenWebUI with Nvidia GPU support, use this command:\ndocker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda Integrating Ollama and Stable Diffusion # Once OpenWebUI is up and running, the next step is to integrate it with Ollama and Stable Diffusion via API connections.\nConnecting Ollama # Open your browser and navigate to: # OpenWebUI Admin Settings # Click to collapse 1.1 http://ip_server_openwebui:3000/admin/settings/\n1.2 Go to Models -\u0026gt; Manage Ollama Models.\n1.3 Enter the URL of your Ollama server\nAdmin Settings If you\u0026rsquo;re running Ollama and OpenWebUI on the same server, this step might be auto-filled.\nConnecting Stable Diffusion # Click to collapse 1.1 Go to Images and find the AUTOMATIC1111 Base URL field.\n1.2 Enter the URL of your Stable Diffusion server:\n1.3 Press the refresh check button next to the URL to verify the connection with the Stable Diffusion server. If everything is working correctly, click Save.\n1.4 You can now adjust the parameters under Set Default Model according to your needs.\nAdmin Settings Voil√†!\nWith OpenWebUI, Ollama, and Stable Diffusion all integrated, you now have a unified interface to interact with your AI models, making it easier to manage and utilize these powerful tools in your home lab.\nConclusion # As I continue to explore and build my AI environment, I plan to enhance my setup with a dedicated GPU in the near future. I\u0026rsquo;m considering an NVIDIA TESLA P40, which should significantly boost performance and allow for more efficient processing. Once I have this upgrade in place, I\u0026rsquo;ll provide updated information and tips on configuring AI tools with GPU support.\nCurrently, with my setup of dual Intel Xeon E5-2620 v2 CPUs at 2.10GHz, the performance is functional but not particularly fast. The experience has been quite interesting, offering valuable insights into AI integration and usage. Here are some performance metrics from my current setup with Ollama and Stable Diffusion:\nResponse Token/s: 0.46 Prompt Token/s: 1.99 Total Duration: 1072376.46 ms (approximately 17 minutes 52 seconds) Load Duration: 61347.1 ms Prompt Eval Count: 33 Prompt Eval Duration: 16571.72 ms Eval Count: 457 Eval Duration: 994411.07 ms These numbers illustrate the impact of running AI models on a CPU-only configuration. Despite the slower performance, the journey has been rewarding and informative.\nThank you for following along with my AI setup journey. I hope these insights and steps will be useful for anyone starting with AI tools in a similar environment. If you have any questions or need further assistance, feel free to reach out!\nFor those interested in GPU setups, here are some useful resources to help with GPU configuration:\nCredits # üìπ TechnoTim - AI Setup. üêô Sean Zheng - Running Llama 3 Model with NVIDIA GPU. ","date":"16 August 2024","externalUrl":null,"permalink":"/blog/run-ai-at-home/","section":"Blog","summary":"In this article, I will go over how I set these tools up on my server as well as their configurations. The difficulties that were there , the solutions I found and useful tips\u0026hellip;","title":"How to Set Up Your Own AI at Home","type":"blog"},{"content":"","date":"16 August 2024","externalUrl":null,"permalink":"/tags/server/","section":"","summary":"","title":"Server","type":"tags"},{"content":"I recently stumbled upon an incredible deal for the Dell PowerEdge R720 server. This powerhouse boasts impressive specifications:\nTechnical Specifications # Server Model: Dell PowerEdge R720 RAM: 192GB Processors: Dual 6-core CPUs with Hyper-Threading (24 threads) iDRAC Controller: iDRAC7 Enterprise Storage: 2 x 2TB SSD \u0026amp; 6 x 300GB SAS drives at 6Gb/s This article covers my journey of integrating this powerhouse into my home lab setup.\nDelivery and Initial Setup # The server arrived in good condition, but I encountered an issue where the front backplane wasn\u0026rsquo;t connected because the included mini SAS cable was too short. I had to search online to find a 59cm mini SAS cable long enough to connect the backplane to the motherboard.\nI was lucky and I found this cable pretty quick thanks to this company in Romania:\ni-Service - MiniSAS\nConfiguring the Cooling System with IPMI-tool # Managing server cooling effectively is crucial for optimal performance and longevity. Using IPMItool, I was able to configure the fans to balance between cooling efficiency and noise levels. Below are the steps and commands I used:\nInstall IPMItool: # sudo apt-get install ipmitool Enable/disable manual fan control # Enable # ipmitool -I lanplus -H ip_addr -U username -P password raw 0x30 0x30 0x01 0x00 Disable # ipmitool -I lanplus -H ip_addr -U username -P password raw 0x30 0x30 0x01 0x01 Set fan speed # ipmitool -I lanplus -H ip_addr -U username -P password raw 0x30 0x30 0x02 0xff 0x14 Consult the table to adapt the speed to your needs # Procent Hexadecimal RPM 10% 0xA ~3,300 RPM 16% 0x10 ~3,900 RPM 20% 0x14 ~4,000 RPM 25% 0x19 ~4,700 RPM 30% 0x1E ~5,400 RPM 40% 0x28 ~7,300 RPM 50% 0x32 ~8,000 RPM 60% 0x3C ~9,400 RPM 70% 0x46 ~10,800 RPM 80% 0x50 ~12,100 RPM 90% 0x5A ~13,300 RPM 100% 0x64 15,000 RPM Monitor fan status # ipmitool sensor | grep -i fan Fine-tuning the fan speeds can drastically reduce the noise levels in a homelab environment, which is often a crucial consideration compared to a data center where noise is less of an issue. Upgrading Firmware via UpdateYODell.net # Keeping firmware up-to-date is essential for security and performance. I upgraded my R720\u0026rsquo;s firmware using the resources available on UpdateYODell.net. Here‚Äôs a step-by-step guide:\nIdentify Your Server‚Äôs Generation: # Visit Wikipedia\u0026rsquo;s Dell PowerEdge page to find your server model and its generation.\nConfigure iDRAC for FTP Update: # Access the iDRAC web interface. Navigate to Maintenance \u0026gt; System Update. Select FTP as the update method and use next settings: Address - ftp.updateyodell.net User Name - dell Password - calvin Path - g11, g12(dell r720), g13, or g14 Click Check for Updates and proceed with the upgrade. Updating the firmware ensures that the server runs smoothly and is protected against known vulnerabilities. It can also bring new features and improvements to your system, which is particularly beneficial in a home lab setting where experimentation and learning are key. Migrating Proxmox Cluster # Migrating my Proxmox cluster to the new server was simplified by utilizing an NFS share on my Synology DS223 in the homelab. Here‚Äôs how I did it:\nMount NFS Share on New Proxmox Server: # mount -t nfs \u0026lt;synology_ip\u0026gt;:/path/to/nfs /mnt/pve/nfs Restore VMs from NFS: # pct restore \u0026lt;vmid\u0026gt; /mnt/pve/nfs/dump/dump.tar Benefits of Using NFS with Proxmox: Using an NFS share for backups and migrations offers several advantages:\nSimplicity: Easy to set up and manage. Efficiency: Fast transfer speeds, especially with a dedicated network. Flexibility: Can easily expand storage as needed. Storage Configuration # In the Dell R720, I configured the storage with two 2TB SSDs in RAID 1 for the operating system and primary applications, and six 300GB SAS drives in RAID 10 for data storage. This setup offers a great balance between performance, redundancy, and storage capacity.\nBenefits of This Storage Setup: # RAID 1 for SSDs: Provides redundancy, ensuring that the OS and critical applications are safe even if one SSD fails. RAID 10 for SAS Drives: Combines the speed benefits of RAID 0 with the redundancy of RAID 1, offering fast read/write speeds and protection against drive failures. Integrating with UPS Using PowerPanel # To protect the server from power outages, integrating it with a UPS (Uninterruptible Power Supply) was crucial. Instead of using NUT, I opted for PowerPanel:\nInstall PowerPanel: # Download and install the PowerPanel software from the CyberPower website.\nDownload # curl -o cyberpowerpowerpanel.deb https://www.cyberpower.com/tw/en/File/GetFileSampleByType?fileId=SU-18070001-06\u0026amp;fileType=Download%20Center\u0026amp;fileSubType=FileOriginal Install # dpkg -i cyberpowerpowerpanel.deb Configure PowerPanel: # I configured PowerPanel in my environment with the following command:\npwrstat -lowbatt -runtime 300 -capacity 35 -active on -cmd /etc/pwrstatd-lowbatt.sh -duration 1 -shutdown on Explanation of the Command: # -lowbatt: Triggers the action when the battery is low. -runtime 300: Triggers the action when the UPS runtime drops below 300 seconds. -capacity 35: Triggers the action when the battery capacity drops below 35%. -active on: Enables the action. -cmd /etc/pwrstatd-lowbatt.sh: Executes the specified script when the condition is met. -duration 1: Specifies the duration in minutes to wait before executing the shutdown. -shutdown on: Initiates a system shutdown when the condition is met. This configuration ensures that my server shuts down gracefully in the event of a power outage, protecting data integrity and preventing hardware damage.\nMonitoring and Management # For monitoring the server\u0026rsquo;s performance and health, I use a combination of Prometheus and Grafana. These tools provide detailed metrics and visualizations, allowing me to keep an eye on resource usage, temperatures, and potential issues.\nBackup Strategy # Having a robust backup strategy is crucial in any lab environment. I use Proxmox\u0026rsquo;s built-in backup tools to create regular snapshots of my VMs, which are then stored on the NFS share. This ensures that I can quickly recover from any data loss or corruption.\nConclusion # Setting up the Dell R720 in my home lab has been an exciting journey. From configuring cooling and upgrading firmware to migrating my Proxmox cluster and integrating with a UPS, every step has enhanced my lab\u0026rsquo;s performance and reliability. Additionally, the advanced network and storage configurations have made my setup more robust and efficient. I hope this guide helps you in your home lab endeavors.\nStay tuned for more updates and experiments in my home lab! Credits # üìπ Tutorial - Dell \u0026amp; HP Server Manual Fan Control. üêô Kenneth Finnegan - Update your old ass Dell servers. üñ•Ô∏è NOiSEA - Cyberpower power panel. ","date":"13 July 2024","externalUrl":null,"permalink":"/blog/dell-r720/","section":"Blog","summary":"I recently stumbled upon an incredible deal for the Dell PowerEdge R720 server. This powerhouse boasts impressive specifications\u0026hellip;","title":"Setting Up Dell R720 Server in the Home Lab","type":"blog"},{"content":"","date":"13 July 2024","externalUrl":null,"permalink":"/tags/tutorial/","section":"","summary":"","title":"Tutorial","type":"tags"},{"content":"","date":"1 July 2024","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 July 2024","externalUrl":null,"permalink":"/authors/merox/","section":"Authors","summary":"","title":"Merox","type":"authors"},{"content":" Seasoned System Administrator and Cybersecurity Engineer, with a profound passion for Linux, Networking, and Security. My career has been a journey through challenging roles, where I've developed a blend of technical prowess and strategic insight. Why merox.dev? # Well, \u0026ldquo;merox.dev\u0026rdquo; does have a personal touch and a little history. It\u0026rsquo;s a fusion of elements that reflect both my background and interests:\nMERO: The first part of the name comes from combining the first letters of my name, \u0026ldquo;ME\u0026rdquo; for Melcher and \u0026ldquo;RO\u0026rdquo; for Robert. This is sort of a wink toward my identity and a touch of personality within the work I do.\nThe \u0026ldquo;X\u0026rdquo; in \u0026ldquo;merox\u0026rdquo; adds some mystery and adaptability to the name. My username was originally based on hexadecimal notation‚Äî\u0026ldquo;mer0x39,\u0026rdquo; where \u0026ldquo;0x39\u0026rdquo; is the number 57, essentially the day and month of my birthday.\nThe idea of merox.dev was to create a full-scale repository including all my experience and knowledge gained within the last ten years working in IT. This site is my personal archive, containing from complex technical solutions to many experiences obtained during projects. A blog section is the newest part where I share my thoughts, trends, and updates on what\u0026rsquo;s new in tech.\nIn essence, merox.dev represents a bit of my path, expertise, and passion for IT; it provides useful resources and information to others within this industry.\nProfessional Journey # Throughout my career, I\u0026rsquo;ve had the privilege of working with industry leaders such as Hella, Atos, and Netex. These experiences have shaped my approach to IT and cybersecurity, allowing me to:\nSpearhead cutting-edge cybersecurity initiatives Optimize complex server environments Align IT strategies with organizational goals Key Achievements # üê≥ Successfully dockerized and migrated a Puppet server from a virtual machine to a Kubernetes cluster, enhancing scalability and resource efficiency üñ•Ô∏è Led the migration and creation of Forescout policies from Windows 7 to Windows 10, ensuring seamless transition and improved security posture üõ°Ô∏è Implemented robust security measures across various network devices including Palo Alto, Fortigate, Cisco, and McAfee, significantly enhancing the organization\u0026rsquo;s cybersecurity infrastructure üíæ Designed and implemented a comprehensive backup solution using Bareos for an entire datacenter, ensuring data integrity and disaster recovery readiness üåê Leveraged CCNA expertise to design and implement complex network infrastructures, optimizing performance and security üöÄ Orchestrated multiple infrastructure modernization projects, improving system reliability and operational efficiency Core Competencies # Linux Systems Networking Security System optimization Infrastructure design Threat analysis Shell scripting Network protocols Penetration testing Performance tuning Troubleshooting Security audits Philosophy \u0026amp; Continuous Learning # I believe in the power of continuous learning and staying ahead of the technological curve. My approach involves:\nConstantly exploring emerging technologies Participating in cybersecurity communities and forums Sharing knowledge through mentoring and tech talks Let\u0026rsquo;s Connect # Interested in collaborating or learning more about my work? I\u0026rsquo;m always open to discussing new opportunities and ideas in the world of IT and cybersecurity.\n","date":"1 July 2024","externalUrl":null,"permalink":"/about/","section":"Merox.dev - IT Blog \u0026 Documentation","summary":"Seasoned System Administrator and Cybersecurity Engineer, with a profound passion for Linux, Networking, and Security.","title":"Who's Merox?","type":"page"},{"content":"","date":"6 April 2024","externalUrl":null,"permalink":"/tags/security/","section":"","summary":"","title":"Security","type":"tags"},{"content":"I\u0026rsquo;ve decided to implement monitoring for my homelab through a cloud virtual machine.\nAs cloud provider I\u0026rsquo;ve opted for Hetzner, but more on that in a future post.\nTo enhance the security of this setup, I\u0026rsquo;ve chosen to establish the cloud VM from Hetzner as the single entry point to my infrastructure. For this purpose, I\u0026rsquo;ve opted to use Tailscale for tunneling, not only for client-to-site but also for site-to-site connectivity.\nInformations provided by tailscale:\n\u0026ldquo;Use site-to-site layer 3 (L3) networking to connect two subnets on your Tailscale network with each other. The two subnets are each required to provide a subnet router but their devices do not need to install Tailscale. This scenario applies to Linux subnet routers only.\u0026rdquo; This scenario will not work on subnets with overlapping CIDR ranges, nor with 4via6 subnet routing. In my case, there are two private subnets without any connectivity between them. Subnet 1 - Homelab: 10.57.57.0/24\nSubnet 2 - Cloudlab: 192.168.57.0/24\nIP addresses of the routers for each subnet: Subnet 1 -\u0026gt; 10.57.57.1 ( pfSense )\nSubnet 2 -\u0026gt; 192.168.57.254 ( Linux VM )\nSetting up Tailscale site-to-site on pfSense Subnet I # Let\u0026rsquo;s dive into the configuration. Due to pfSense being based on FreeBSD and Tailscale not offering as much support for pfSense as for other platforms, this configuration is a bit trickier. But let\u0026rsquo;s see how it looks.\nInstall tailscale on pfSense # Navigate to Package Manager: Go to System \u0026gt; Package Manager in the pfSense web interface.\nInstall Package: Click on the \u0026ldquo;Available Packages\u0026rdquo; tab. Search for tailscale and click \u0026ldquo;Install\u0026rdquo;.\nConfigure tailscale on pfSense # Navigate to Tailscale: VPN -\u0026gt; Tailscale\nAuthentication # Copy auth-key from https://login.tailscale.com/admin/settings/keys Generate Auth keys # Check: \u0026ldquo;Enable tailscale\u0026rdquo; Listen port: leave it as it is Check: Accept Subnet Routes Optional check: Advertise Exit Node Advertised Routes: 10.57.57.0/24 Tricky part: Outbound NAT Rules # Navigate to Firewall-\u0026gt; NAT-\u0026gt; Outbound\nMake sure Outbound NAT Mode is configured to be configured as # Hybrid Outbound NAT\nCreate next manual mapping # Interface: Tailscale Address Family: IPV4+IPV6 Protocol: Any Source Network or Alias: 10.57.57.0/24 Destination: Any This part is broken from last update [ 23.09.1 ] so NAT Alias is missing. Workaround:\nTranslation section:\nAddress: Network or Alias Put the tailscale ip address 100.xx.xx.xx/32\nThis is how should look like:\nConfigure tailscale site-to-site on Linux VM Subnet II # Install tailscale and activate routing: # 1 curl -sSL https://tailscale.com/install.sh | sh #Install tailscale 2 echo \u0026#39;net.ipv4.ip_forward = 1\u0026#39; | sudo tee -a /etc/sysctl.conf #Activate routing for IPv4 3 echo \u0026#39;net.ipv6.conf.all.forwarding = 1\u0026#39; | sudo tee -a /etc/sysctl.conf #Activate routing for IPv6 4 sudo sysctl -p /etc/sysctl.conf # Apply routing configuration at kernel level On the 192.168.57.254 device, advertise routes for 192.168.57.0/24: # 1 tailscale up --advertise-routes=192.168.57.0/24 --snat-subnet-routes=false --accept-routes Command explained:\n\u0026ndash;advertise-routes: Exposes the physical subnet routes to your entire Tailscale network.\n\u0026ndash;snat-subnet-routes=false: Disables source NAT. In normal operations, a subnet device will see the traffic originating from the subnet router. This simplifies routing, but does not allow traversing multiple networks. By disabling source NAT, the end machine sees the LAN IP address of the originating machine as the source.\n\u0026ndash;accept-routes: Accepts the advertised route of the other subnet router, as well as any other nodes that are subnet routers.\nEnable subnet routes from the admin console # This step is not required if using autoApprovers. Open the Machines page of the admin console, and locate the devices that you configured as subnet routers. You can look for the Subnets badge in the machines list, or use the property:subnet filter to see all devices advertising subnet routes. For each device that you need to approve, click the ellipsis icon menu at the end of the table, and select Edit route settings. In the Edit route settings panel, approve the device.\nThe Tailscale side of the routing is complete. Credits # üìù Tailscale - Seamless networking for secure connections. üìπ Christian McDonald - YouTube Channel. ","date":"6 April 2024","externalUrl":null,"permalink":"/blog/tailscale-site-to-site/","section":"Blog","summary":"I\u0026rsquo;ve decided to implement monitoring for my homelab through a cloud virtual machine. As cloud provider I\u0026rsquo;ve opted for Hetzner\u0026hellip;","title":"Tailscale site-to-site pfSense - Linux","type":"blog"},{"content":"For a long time, I\u0026rsquo;ve been on the hunt for a comprehensive and well-crafted tutorial to deploy a media server on my Kubernetes cluster. This media server stack includes Jellyfin, Radarr, Sonarr, Jackett, and qBittorrent. Let\u0026rsquo;s briefly dive into what each component brings to our setup\nApplication Description Jellyfin An open-source media system that provides a way to manage and stream your media library across various devices. Radarr A movie collection manager for Usenet and BitTorrent users. It automates the process of searching for movies, downloading, and managing your movie library. Sonarr Similar to Radarr but for TV shows. It keeps track of your series, downloads new episodes, and manages your collection with ease. Jackett Acts as a proxy server, translating queries from other apps (like Sonarr or Radarr) into queries that can be understood by a wide array of torrent search engines. qBittorrent A powerful BitTorrent client that handles your downloads. Paired with Jackett, it streamlines finding and downloading media content. Gluetun A lightweight, open-source VPN client for Docker environments, supporting multiple VPN providers to secure and manage internet connections across containerized applications. It ensures privacy and seamless network security with easy configuration and integration. The configuration for these applications is hosted on Longhorn storage, ensuring resilience and ease of management, while the media (movies, shows, books, etc.) is stored on a Synology NAS DS223. The NAS location is utilized as a Persistent Volume (PV) through NFS 4.1 by Kubernetes.\nIn this tutorial, you\u0026rsquo;ll find the Kubernetes configuration for each necessary component to set up, install, and secure each service used by the media server.\nSynology NAS NFS Setup for Kubernetes # If you use Synology NAS, this is the rule I created for my NFS share which will be mounted on kubernetes side. Let\u0026rsquo;s start step by step.\nConfiguring PVC and PV for NFS Share # Media # Create nfs-media-pv-and-pvc.yaml:\n1apiVersion: v1 2kind: PersistentVolume 3metadata: 4 name: jellyfin-videos 5spec: 6 capacity: 7 storage: 400Gi 8 accessModes: 9 - ReadWriteOnce 10 nfs: 11 path: /volume1/server/k3s/media 12 server: storage.merox.cloud 13 persistentVolumeReclaimPolicy: Retain 14 mountOptions: 15 - hard 16 - nfsvers=3 17 storageClassName: \u0026#34;\u0026#34; 18# Persistent Volume spec including capacity, access modes, NFS path, and server details follow 19--- 20apiVersion: v1 21kind: PersistentVolumeClaim 22metadata: 23 name: jellyfin-videos 24 namespace: media 25spec: 26 accessModes: 27 - ReadWriteOnce 28 resources: 29 requests: 30 storage: 400Gi 31 volumeName: jellyfin-videos 32 storageClassName: \u0026#34;\u0026#34; 33# Persistent Volume Claim spec including access modes, resources requests, and storage class name follow Apply with:\nkubectl apply -f nfs-media-pv-and-pvc.yaml Download # Create nfs-download-pv-and-pvc.yaml:\n1apiVersion: v1 2kind: PersistentVolume 3metadata: 4 name: qbitt-download 5spec: 6 capacity: 7 storage: 400Gi 8 accessModes: 9 - ReadWriteOnce 10 nfs: 11 path: /volume1/server/k3s/media/download 12 server: storage.merox.cloud 13 persistentVolumeReclaimPolicy: Retain 14 mountOptions: 15 - hard 16 - nfsvers=3 17 storageClassName: \u0026#34;\u0026#34; 18# Persistent Volume spec including capacity, access modes, NFS path, and server details follow 19--- 20apiVersion: v1 21kind: PersistentVolumeClaim 22metadata: 23 name: qbitt-download 24 namespace: media 25spec: 26 accessModes: 27 - ReadWriteOnce 28 resources: 29 requests: 30 storage: 400Gi 31 volumeName: qbitt-download 32 storageClassName: \u0026#34;\u0026#34; 33# Persistent Volume Claim spec including access modes, resources requests, and storage class name follow Apply with: kubectl apply -f nfs-download-pv-and-pvc.yaml\nConfiguring Longhorn PVC for Each Application # Create app-config-pvc.yaml:\n1apiVersion: v1 2kind: PersistentVolumeClaim 3metadata: 4 name: app # radarr for example 5 namespace: media 6spec: 7 accessModes: 8 - ReadWriteOnce 9 storageClassName: longhorn 10 resources: 11 requests: 12 storage: 5Gi 13# Persistent Volume Claim spec including access modes, storage class name, and resources requests follow Apply with: kubectl apply -f app-config-pvc.yaml\nThis type of configuration needs to be generated for each application: Jellyfin, Sonarr, Radarr, Jackett, qBittorrent. Deploying each application # Jellyfin # Jellyfin serves as our media streaming platform, providing access to movies, TV shows, and other media across various devices. Here\u0026rsquo;s how to deploy it\nCreate specific yaml for each file, for example: radarr-deployment.yaml Apply with\nkubectl apply -f radarr-deployment.yaml 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: jellyfin 5 namespace: media 6spec: 7 replicas: 1 8 selector: 9 matchLabels: 10 app: jellyfin 11 template: 12 metadata: 13 labels: 14 app: jellyfin 15 spec: 16 containers: 17 - name: jellyfin 18 image: jellyfin/jellyfin 19 volumeMounts: 20 - name: config 21 mountPath: /config 22 - name: videos 23 mountPath: /data/videos 24 ports: 25 - containerPort: 8096 26 volumes: 27 - name: config 28 persistentVolumeClaim: 29 claimName: jellyfin-config 30 - name: videos 31 persistentVolumeClaim: 32 claimName: jellyfin-videos Sonarr # Sonarr automates TV show downloads, managing our series collection efficiently.\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: sonarr 5 namespace: media 6spec: 7 replicas: 1 8 selector: 9 matchLabels: 10 app: sonarr 11 template: 12 metadata: 13 labels: 14 app: sonarr 15 spec: 16 containers: 17 - name: sonarr 18 image: linuxserver/sonarr 19 env: 20 - name: PUID 21 value: \u0026#34;1057\u0026#34; 22 - name: PGID 23 value: \u0026#34;1056\u0026#34; 24 volumeMounts: 25 - name: config 26 mountPath: /config 27 - name: videos 28 mountPath: /tv 29 - name: downloads 30 mountPath: /downloads 31 ports: 32 - containerPort: 8989 33 volumes: 34 - name: config 35 persistentVolumeClaim: 36 claimName: sonarr-config 37 - name: videos 38 persistentVolumeClaim: 39 claimName: jellyfin-videos 40 - name: downloads 41 persistentVolumeClaim: 42 claimName: qbitt-download Radarr # Radarr works like Sonarr but focuses on movies, keeping our film library organized and up-to-date.\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: radarr 5 namespace: media 6spec: 7 replicas: 1 8 selector: 9 matchLabels: 10 app: radarr 11 template: 12 metadata: 13 labels: 14 app: radarr 15 spec: 16 containers: 17 - name: radarr 18 image: linuxserver/radarr 19 env: 20 - name: PUID 21 value: \u0026#34;1057\u0026#34; 22 - name: PGID 23 value: \u0026#34;1056\u0026#34; 24 volumeMounts: 25 - name: config 26 mountPath: /config 27 - name: videos 28 mountPath: /movies 29 - name: downloads 30 mountPath: /downloads 31 ports: 32 - containerPort: 7878 33 volumes: 34 - name: config 35 persistentVolumeClaim: 36 claimName: radarr-config 37 - name: videos 38 persistentVolumeClaim: 39 claimName: jellyfin-videos 40 - name: downloads 41 persistentVolumeClaim: 42 claimName: qbitt-download Jackett # Jackett acts as a bridge between torrent search engines and our media management tools, enhancing their capabilities.\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: jackett 5 namespace: media 6spec: 7 replicas: 1 8 selector: 9 matchLabels: 10 app: jackett 11 template: 12 metadata: 13 labels: 14 app: jackett 15 spec: 16 containers: 17 - name: jackett 18 image: linuxserver/jackett 19 env: 20 - name: PUID 21 value: \u0026#34;1057\u0026#34; 22 - name: PGID 23 value: \u0026#34;1056\u0026#34; 24 volumeMounts: 25 - name: config 26 mountPath: /config 27 ports: 28 - containerPort: 9117 29 volumes: 30 - name: config 31 persistentVolumeClaim: 32 claimName: jackett-config qBittorrent # 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: qbittorrent 5 namespace: media 6spec: 7 replicas: 1 8 selector: 9 matchLabels: 10 app: qbittorrent 11 template: 12 metadata: 13 labels: 14 app: qbittorrent 15 spec: 16 containers: 17 - name: qbittorrent 18 image: linuxserver/qbittorrent 19 resources: 20 limits: 21 memory: \u0026#34;2Gi\u0026#34; 22 requests: 23 memory: \u0026#34;512Mi\u0026#34; 24 env: 25 - name: PUID 26 value: \u0026#34;1057\u0026#34; 27 - name: PGID 28 value: \u0026#34;1056\u0026#34; 29 volumeMounts: 30 - name: config 31 mountPath: /config 32 - name: downloads 33 mountPath: /downloads 34 ports: 35 - containerPort: 8080 36 volumes: 37 - name: config 38 persistentVolumeClaim: 39 claimName: qbitt-config 40 - name: downloads 41 persistentVolumeClaim: 42 claimName: qbitt-download qBittorrent with Gluetun # 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: qbittorrent 5 namespace: media 6spec: 7 replicas: 1 8 selector: 9 matchLabels: 10 app: qbittorrent 11 template: 12 metadata: 13 labels: 14 app: qbittorrent 15 spec: 16 containers: 17 - name: qbittorrent 18 image: linuxserver/qbittorrent 19 resources: 20 limits: 21 memory: \u0026#34;2Gi\u0026#34; 22 requests: 23 memory: \u0026#34;512Mi\u0026#34; 24 env: 25 - name: PUID 26 value: \u0026#34;1057\u0026#34; 27 - name: PGID 28 value: \u0026#34;1056\u0026#34; 29 volumeMounts: 30 - name: config 31 mountPath: /config 32 - name: downloads 33 mountPath: /downloads 34 ports: 35 - containerPort: 8080 36 37 - name: gluetun 38 image: qmcgaw/gluetun 39 env: 40 - name: VPNSP 41 value: \u0026#34;protonvpn\u0026#34; 42 - name: OPENVPN_USER 43 valueFrom: 44 secretKeyRef: 45 name: protonvpn-secrets 46 key: PROTONVPN_USER 47 - name: OPENVPN_PASSWORD 48 valueFrom: 49 secretKeyRef: 50 name: protonvpn-secrets 51 key: PROTONVPN_PASSWORD 52 - name: COUNTRY 53 value: \u0026#34;Germany\u0026#34; 54 securityContext: 55 capabilities: 56 add: 57 - NET_ADMIN 58 volumeMounts: 59 - name: gluetun-config 60 mountPath: /gluetun 61 62 volumes: 63 - name: config 64 persistentVolumeClaim: 65 claimName: qbitt-config 66 - name: downloads 67 persistentVolumeClaim: 68 claimName: qbitt-download 69 - name: gluetun-config 70 persistentVolumeClaim: 71 claimName: gluetun-config I\u0026rsquo;ve chosen to use ProtonVPN due to their security policy and because they do not collect/store data, but also because of the speeds and diverse settings, all at a very good price Creating ClusterIP Services # For our media server applications to communicate efficiently within the Kubernetes cluster without exposing them directly to the external network, we utilize ClusterIP services.\nTo set this up, we create a app-service.yaml for each application (taking Radarr as an example here):\ncreate app-service.yaml 1apiVersion: v1 2kind: Service 3metadata: 4 name: app #radarr for example 5 namespace: media 6spec: 7 type: ClusterIP 8 ports: 9 - port: 80 10 targetPort: 7878 11 selector: 12 app: app #radarr for example kubectl apply -f app-service.yaml\nCreating middleware for Traefik # For enhanced security and to ensure smooth functioning with Traefik, we define middleware:\nThe middleware, named default-headers-media, is configured in the media namespace. It sets various security headers, including XSS protection and options to prevent MIME sniffing, among others. Create default-headers-media.yaml\n1apiVersion: traefik.containo.us/v1alpha1 2kind: Middleware 3metadata: 4 name: default-headers-media 5 namespace: media 6spec: 7 headers: 8 browserXssFilter: true 9 contentTypeNosniff: true 10 forceSTSHeader: true 11 stsIncludeSubdomains: true 12 stsPreload: true 13 stsSeconds: 15552000 14 customFrameOptionsValue: SAMEORIGIN 15 customRequestHeaders: 16 X-Forwarded-Proto: https Apply with: kubectl apply -f default-headers-media.yaml\nCreating Ingress Route for Each Application # To expose each application securely, we create IngressRoutes using Traefik:\nAn IngressRoute for the application (such as Radarr) is defined, which uses the traefik-external ingress class. It listens on the websecure entry point and routes traffic based on the host (movies.merox.cloud in this example, replace with your domain). The middleware default-headers-media is applied to enhance security. TLS configuration is included, referencing a secret that contains the SSL/TLS certificate. Create app-ingress-route.yaml\n1apiVersion: traefik.containo.us/v1alpha1 2kind: IngressRoute 3metadata: 4 name: app #radarr for example 5 namespace: media 6 annotations: 7 kubernetes.io/ingress.class: traefik-external 8spec: 9 entryPoints: 10 - websecure 11 routes: 12 - match: Host(`movies.merox.cloud`) # change to your domain 13 kind: Rule 14 services: 15 - name: app #radarr for example 16 port: 80 17 - match: Host(`movies.merox.cloud`) # change to your domain 18 kind: Rule 19 services: 20 - name: app #radarr for example 21 port: 80 22 middlewares: 23 - name: default-headers-media 24 tls: 25 secretName: mycert-tls # change to your cert name Apply with: kubectl apply -f app-ingress-route.yaml\nDon\u0026rsquo;t forget: You must create the host declared in your IngressRoute in your DNS server(s). Q\u0026amp;A # Q: Why use a ClusterIP service? A: Because we will be using Traefik as an ingress controller to expose it to the local network/internet with SSL/TLS certificates. Q: Can I download all manifest files from anywhere? A: SURE! The link is at the end of this page :) This concludes the necessary steps and configurations to deploy a resilient media server in a Kubernetes cluster successfully.\nManifest files # Just copy and deploy all you need in no time\nAll manifest files üîó ","date":"6 March 2024","externalUrl":null,"permalink":"/blog/kubernetes-media-server/","section":"Blog","summary":"For a long time, I\u0026rsquo;ve been on the hunt for a comprehensive and well-crafted tutorial to deploy a media server on my Kubernetes cluster.","title":"Deploying a Kubernetes-Based Media Server","type":"blog"},{"content":"","date":"6 March 2024","externalUrl":null,"permalink":"/tags/media-server/","section":"","summary":"","title":"Media-Server","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"","summary":"","title":"","type":"categories"},{"content":" Privacy Policy Last updated: October 19, 2024\nThis Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.\nWe use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. This Privacy Policy has been created with the help of the Free Privacy Policy Generator.\nInterpretation and Definitions Interpretation The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\nDefinitions For the purposes of this Privacy Policy:\nAccount means a unique account created for You to access our Service or parts of our Service. Affiliate means an entity that controls, is controlled by or is under common control with a party, where \u0026ldquo;control\u0026rdquo; means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority. Company (referred to as either \u0026ldquo;the Company\u0026rdquo;, \u0026ldquo;We\u0026rdquo;, \u0026ldquo;Us\u0026rdquo; or \u0026ldquo;Our\u0026rdquo; in this Agreement) refers to Merox. Cookies are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses. Country refers to: Romania Device means any device that can access the Service such as a computer, a cellphone or a digital tablet. Personal Data is any information that relates to an identified or identifiable individual. Service refers to the Website. Service Provider means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used. Usage Data refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit). Website refers to Merox, accessible from https://merox.dev You means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable. Collecting and Using Your Personal Data Types of Data Collected Personal Data While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:\nEmail address Usage Data Usage Data Usage Data is collected automatically when using the Service.\nUsage Data may include information such as Your Device\u0026rsquo;s Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\nWhen You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.\nWe may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.\nTracking Technologies and Cookies We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:\nCookies or Browser Cookies. A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies. Web Beacons. Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity). Cookies can be \u0026ldquo;Persistent\u0026rdquo; or \u0026ldquo;Session\u0026rdquo; Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser.\nWe use both Session and Persistent Cookies for the purposes set out below:\nNecessary / Essential Cookies\nType: Session Cookies Administered by: Us Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services. Cookies Policy / Notice Acceptance Cookies\nType: Persistent Cookies Administered by: Us Purpose: These Cookies identify if users have accepted the use of cookies on the Website. Functionality Cookies\nType: Persistent Cookies Administered by: Us Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website. For more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.\nUse of Your Personal Data The Company may use Personal Data for the following purposes:\nTo provide and maintain our Service, including to monitor the usage of our Service. To manage Your Account: to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user. For the performance of a contract: the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service. To contact You: To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application\u0026rsquo;s push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation. To provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information. To manage Your requests: To attend and manage Your requests to Us. For business transfers: We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred. For other purposes: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience. We may share Your personal information in the following situations:\nWith Service Providers: We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You. For business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company. With Affiliates: We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us. With business partners: We may share Your information with Our business partners to offer You certain products, services or promotions. With other users: when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside. With Your consent: We may disclose Your personal information for any other purpose with Your consent. Retention of Your Personal Data The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\nThe Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.\nTransfer of Your Personal Data Your information, including Personal Data, is processed at the Company\u0026rsquo;s operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to ‚Äî and maintained on ‚Äî computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.\nYour consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.\nThe Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.\nDelete Your Personal Data You have the right to delete or request that We assist in deleting the Personal Data that We have collected about You.\nOur Service may give You the ability to delete certain information about You from within the Service.\nYou may update, amend, or delete Your information at any time by signing in to Your Account, if you have one, and visiting the account settings section that allows you to manage Your personal information. You may also contact Us to request access to, correct, or delete any personal information that You have provided to Us.\nPlease note, however, that We may need to retain certain information when we have a legal obligation or lawful basis to do so.\nDisclosure of Your Personal Data Business Transactions If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.\nLaw enforcement Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\nOther legal requirements The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:\nComply with a legal obligation Protect and defend the rights or property of the Company Prevent or investigate possible wrongdoing in connection with the Service Protect the personal safety of Users of the Service or the public Protect against legal liability Security of Your Personal Data The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.\nLinks to Other Websites Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party\u0026rsquo;s site. We strongly advise You to review the Privacy Policy of every site You visit.\nWe have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.\nChanges to this Privacy Policy We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.\nWe will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the \u0026ldquo;Last updated\u0026rdquo; date at the top of this Privacy Policy.\nYou are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.\nContact Us If you have any questions about this Privacy Policy, You can contact us:\nBy email: hello@merox.dev ","externalUrl":null,"permalink":"/privacy/","section":"Merox.dev - IT Blog \u0026 Documentation","summary":"\u003ch1 class=\"relative group\"\u003ePrivacy Policy \n    \u003cdiv id=\"privacy-policy\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eLast updated: October 19, 2024\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThis Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.\u003c/p\u003e","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"","summary":"","title":"","type":"tags"},{"content":" Terms and Conditions Last updated: October 21, 2024\nPlease read these terms and conditions carefully before using Our Service.\nInterpretation and Definitions Interpretation The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\nDefinitions For the purposes of these Terms and Conditions:\nAffiliate means an entity that controls, is controlled by or is under common control with a party, where \u0026ldquo;control\u0026rdquo; means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority. Country refers to: Romania. Company (referred to as either \u0026ldquo;the Company\u0026rdquo;, \u0026ldquo;We\u0026rdquo;, \u0026ldquo;Us\u0026rdquo; or \u0026ldquo;Our\u0026rdquo; in this Agreement) refers to Merox. Device means any device that can access the Service such as a computer, a cellphone or a digital tablet. Service refers to the Website. Terms and Conditions (also referred as \u0026ldquo;Terms\u0026rdquo;) mean these Terms and Conditions that form the entire agreement between You and the Company regarding the use of the Service. This Terms and Conditions agreement has been created with the help of the Free Terms and Conditions Generator. Third-party Social Media Service means any services or content (including data, information, products or services) provided by a third-party that may be displayed, included or made available by the Service. Website refers to Merox, accessible from https://merox.dev. You means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable. Acknowledgment These are the Terms and Conditions governing the use of this Service and the agreement that operates between You and the Company. These Terms and Conditions set out the rights and obligations of all users regarding the use of the Service.\nYour access to and use of the Service is conditioned on Your acceptance of and compliance with these Terms and Conditions. These Terms and Conditions apply to all visitors, users and others who access or use the Service.\nBy accessing or using the Service You agree to be bound by these Terms and Conditions. If You disagree with any part of these Terms and Conditions then You may not access the Service.\nYou represent that you are over the age of 18. The Company does not permit those under 18 to use the Service.\nYour access to and use of the Service is also conditioned on Your acceptance of and compliance with the Privacy Policy of the Company. Our Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your personal information when You use the Application or the Website and tells You about Your privacy rights and how the law protects You. Please read Our Privacy Policy carefully before using Our Service.\nLinks to Other Websites Our Service may contain links to third-party websites or services that are not owned or controlled by the Company.\nThe Company has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third-party websites or services. You further acknowledge and agree that the Company shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with the use of or reliance on any such content, goods or services available on or through any such websites or services.\nWe strongly advise You to read the terms and conditions and privacy policies of any third-party websites or services that You visit.\nTermination We may terminate or suspend Your access immediately, without prior notice or liability, for any reason whatsoever, including without limitation if You breach these Terms and Conditions.\nUpon termination, Your right to use the Service will cease immediately.\nLimitation of Liability Notwithstanding any damages that You might incur, the entire liability of the Company and any of its suppliers under any provision of this Terms and Your exclusive remedy for all of the foregoing shall be limited to the amount actually paid by You through the Service or 100 USD if You haven\u0026rsquo;t purchased anything through the Service.\nTo the maximum extent permitted by applicable law, in no event shall the Company or its suppliers be liable for any special, incidental, indirect, or consequential damages whatsoever (including, but not limited to, damages for loss of profits, loss of data or other information, for business interruption, for personal injury, loss of privacy arising out of or in any way related to the use of or inability to use the Service, third-party software and/or third-party hardware used with the Service, or otherwise in connection with any provision of this Terms), even if the Company or any supplier has been advised of the possibility of such damages and even if the remedy fails of its essential purpose.\nSome states do not allow the exclusion of implied warranties or limitation of liability for incidental or consequential damages, which means that some of the above limitations may not apply. In these states, each party\u0026rsquo;s liability will be limited to the greatest extent permitted by law.\n\u0026ldquo;AS IS\u0026rdquo; and \u0026ldquo;AS AVAILABLE\u0026rdquo; Disclaimer The Service is provided to You \u0026ldquo;AS IS\u0026rdquo; and \u0026ldquo;AS AVAILABLE\u0026rdquo; and with all faults and defects without warranty of any kind. To the maximum extent permitted under applicable law, the Company, on its own behalf and on behalf of its Affiliates and its and their respective licensors and service providers, expressly disclaims all warranties, whether express, implied, statutory or otherwise, with respect to the Service, including all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement, and warranties that may arise out of course of dealing, course of performance, usage or trade practice.\nWithout limitation to the foregoing, the Company provides no warranty or undertaking, and makes no representation of any kind that the Service will meet Your requirements, achieve any intended results, be compatible or work with any other software, applications, systems or services, operate without interruption, meet any performance or reliability standards or be error-free or that any errors or defects can or will be corrected.\nSome jurisdictions do not allow the exclusion of certain types of warranties or limitations on applicable statutory rights of a consumer, so some or all of the above exclusions and limitations may not apply to You. But in such a case the exclusions and limitations set forth in this section shall be applied to the greatest extent enforceable under applicable law.\nUse of Newsletter, Disqus, Google Analytics, and Google AdSense Our Service utilizes third-party tools to enhance user experience and provide valuable content. These tools include:\nNewsletter: By subscribing to our newsletter, you agree to receive occasional emails from us, which may include updates, blog posts, and promotional content. You can unsubscribe at any time using the link provided in each email.\nDisqus: We use Disqus for the commenting system on our blog. Disqus collects certain information when you comment, such as your username, IP address, and any other information you choose to provide. Please refer to Disqus‚Äôs privacy policy for more details on how they handle your data.\nGoogle Analytics: We use Google Analytics to analyze how visitors interact with our website. Google Analytics collects data such as your IP address, browser type, and pages visited. This information helps us improve the website‚Äôs performance and understand user behavior. For more information, please review Google‚Äôs privacy policy.\nGoogle AdSense: Our website uses Google AdSense to display ads and help support the website. Google AdSense may use cookies and web beacons to collect data about your interactions with these ads. For more information on how Google collects and processes data, please review Google‚Äôs privacy policy.\nBy using our Service, you acknowledge and consent to the use of these third-party services as described above.\nGoverning Law The laws of the Country, excluding its conflicts of law rules, shall govern this Terms and Your use of the Service. Your use of the Application may also be subject to other local, state, national, or international laws.\nDisputes Resolution If You have any concern or dispute about the Service, You agree to first try to resolve the dispute informally by contacting the Company.\nFor European Union (EU) Users If You are a European Union consumer, you will benefit from any mandatory provisions of the law of the country in which You are resident.\nUnited States Legal Compliance You represent and warrant that (i) You are not located in a country that is subject to the United States government embargo, or that has been designated by the United States government as a \u0026ldquo;terrorist supporting\u0026rdquo; country, and (ii) You are not listed on any United States government list of prohibited or restricted parties.\nSeverability and Waiver Severability If any provision of these Terms is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.\nWaiver Except as provided herein, the failure to exercise a right or to require performance of an obligation under these Terms shall not affect a party\u0026rsquo;s ability to exercise such right or require such performance at any time thereafter nor shall the waiver of a breach constitute a waiver of any subsequent breach.\nTranslation Interpretation These Terms and Conditions may have been translated if We have made them available to You on our Service. You agree that the original English text shall prevail in the case of a dispute.\nChanges to These Terms and Conditions We reserve the right, at Our sole discretion, to modify or replace these Terms at any time. If a revision is material We will make reasonable efforts to provide at least 30 days\u0026rsquo; notice prior to any new terms taking effect. What constitutes a material change will be determined at Our sole discretion.\nBy continuing to access or use Our Service after those revisions become effective, You agree to be bound by the revised terms. If You do not agree to the new terms, in whole or in part, please stop using the website and the Service.\nContact Us If you have any questions about these Terms and Conditions, You can contact us:\nBy email: hello@merox.dev ","externalUrl":null,"permalink":"/terms-and-conditions/","section":"Merox.dev - IT Blog \u0026 Documentation","summary":"\u003ch1 class=\"relative group\"\u003eTerms and Conditions \n    \u003cdiv id=\"terms-and-conditions\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eLast updated:\u003c/strong\u003e October 21, 2024\u003c/p\u003e\n\u003cp\u003ePlease read these terms and conditions carefully before using Our Service.\u003c/p\u003e","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":"","title":"Blog","type":"blog"},{"content":"Writing about IT infrastructure‚Äîhomelab and real experiences, lessons learned, and insights from my journey in tech.\n","externalUrl":null,"permalink":"/","section":"Merox.dev - IT Blog \u0026 Documentation","summary":"\u003cp\u003e\u003cstrong\u003eWriting about IT infrastructure‚Äîhomelab and real experiences, lessons learned, and insights from my journey in tech.\u003c/strong\u003e\u003c/p\u003e","title":"Merox.dev - IT Blog \u0026 Documentation","type":"page"}]